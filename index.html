<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"chenzhongyao.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="chenzhongyao&#39;s website">
<meta property="og:url" content="https://chenzhongyao.github.io/index.html">
<meta property="og:site_name" content="chenzhongyao&#39;s website">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="chennan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://chenzhongyao.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>chenzhongyao's website</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">chenzhongyao's website</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2021/03/15/%E5%88%9B%E5%BB%BA%E6%96%87%E7%AB%A0%E7%9A%84%E6%AD%A5%E9%AA%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/15/%E5%88%9B%E5%BB%BA%E6%96%87%E7%AB%A0%E7%9A%84%E6%AD%A5%E9%AA%A4/" class="post-title-link" itemprop="url">创建文章的步骤</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-03-15 09:27:16 / 修改时间：09:36:12" itemprop="dateCreated datePublished" datetime="2021-03-15T09:27:16+08:00">2021-03-15</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>777</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="新建文章具体步骤"><a href="#新建文章具体步骤" class="headerlink" title="新建文章具体步骤"></a>新建文章具体步骤</h3><ul>
<li>step 1: <code>hexo new &quot;文章题目&quot;</code> #该操作会在source/_posts下新建文件夹“文章题目”以及“文章题目.md”. 在这两个文件夹下编辑文章即可</li>
<li>step 2: <code>hexo s -g</code> </li>
<li>step 3: <code>hexo d -g</code> #部署到GitHub.  地址： <a href="http://chenzhongyao.github.io">http://chenzhongyao.github.io</a></li>
</ul>
<h3 id="hexo-命令介绍"><a href="#hexo-命令介绍" class="headerlink" title="hexo 命令介绍"></a>hexo 命令介绍</h3><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><figure class="highlight plain"><figcaption><span>new "postName"``` #新建文章</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo new page &quot;pageName&quot; &#96;&#96;&#96;#新建页面</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo generate&#96;&#96;&#96; #生成静态页面至public目录</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo server&#96;&#96;&#96; #开启预览访问端口（默认端口4000，&#39;ctrl + c&#39;关闭server）</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo deploy&#96;&#96;&#96; #部署到GitHub</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo help&#96;&#96;&#96;  # 查看帮助</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo version&#96;&#96;&#96;  #查看Hexo的版本</span><br><span class="line"></span><br><span class="line">#### 缩写命令：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo n&#96;&#96;&#96; &#x3D;&#x3D; &#96;&#96;&#96;hexo new</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>g``` </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#96;&#96;&#96;hexo s&#96;&#96;&#96; &#x3D;&#x3D; &#96;&#96;&#96;hexo server</span><br></pre></td></tr></table></figure>
<p><code>hexo d</code> == <code>hexo deploy</code></p>
<h4 id="组合命令："><a href="#组合命令：" class="headerlink" title="组合命令："></a>组合命令：</h4><p><code>hexo s -g</code> #生成并本地预览</p>
<p><code>hexo d -g</code> #生成并上传</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2021/03/14/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/14/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Docker常用命令总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-03-14 19:23:20" itemprop="dateCreated datePublished" datetime="2021-03-14T19:23:20+08:00">2021-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-15 09:19:51" itemprop="dateModified" datetime="2021-03-15T09:19:51+08:00">2021-03-15</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-docker镜像下载"><a href="#1-docker镜像下载" class="headerlink" title="1. docker镜像下载"></a>1. docker镜像下载</h3><p>在 <a href="https://hub.docker.com/_/ubuntu" target="_blank" rel="noopener">https://hub.docker.com/_/ubuntu</a> 寻找想要的镜像，用<code>docker pull</code> 拉下来</p>
<p>例如下载opencv4-python3的镜像：<code>docker pull ajeetraina/opencv4-python3</code> .</p>
<h3 id="2-docker镜像、容器的删除"><a href="#2-docker镜像、容器的删除" class="headerlink" title="2. docker镜像、容器的删除"></a>2. docker镜像、容器的删除</h3><p>查看本地镜像: <code>docker images</code></p>
<p>删除镜像： <code>docker rmi image_ID</code> (必须清理掉该镜像下所有处于终止状态的容器，可使用 <code>docker ps -a</code> 查看所有处于终止状态的容器) </p>
<p>删除容器： <code>docker rm container_ID</code></p>
<p>注意：删除镜像时需要把该镜像的容器全部删除，分为3步</p>
<ul>
<li><p>首先，查看已经退出的容器： <code>docker ps -a</code> </p>
</li>
<li><p>然后，删除退出的容器：<code>docker rm container_id</code> </p>
</li>
<li><p>最后，删除镜像 <code>docker rmi image_id</code> </p>
</li>
<li><p>前两步可以直接用 <code>docker container prune</code> 直接清理掉所有处于终止状态的容器</p>
</li>
</ul>
<h3 id="3-镜像的运行"><a href="#3-镜像的运行" class="headerlink" title="3. 镜像的运行"></a>3. 镜像的运行</h3><h4 id="3-1-docker-的参数："><a href="#3-1-docker-的参数：" class="headerlink" title="3.1 docker 的参数："></a>3.1 docker 的参数：</h4><ul>
<li><p>-i： 交互式操作。</p>
</li>
<li><p>-t： 终端 （<code>-it</code>  同时使用可以让 docker 运行的容器实现”对话”的能力）</p>
</li>
<li>-d： 参数默认不会进入容器，想要进入容器需要使用指令 docker exec(后边会讲到)</li>
<li>-v：挂在本地目录到容器目录，格式：<strong>本地目录:容器目录</strong>，使用方法<code>docker run -it -v /d:/home image_id /bin/bash</code></li>
<li>—rm: 在Docker容器退出时，默认容器内部的文件系统仍然被保留，以方便调试并保留用户数据(即通过 <code>docker ps -a</code> 查看到的处于终止状态的容器，还可以通过docker start container_ID再次进入)。 因而可以在容器启动时设置—rm选项，这样在容器退出时就能够自动清理容器内部的文件系统。使用示例：<code>docker run -it --rm -v /d:/home image_ID /bin/bash</code> 。显然，—rm选项不能与-d同时使用</li>
</ul>
<h4 id="3-2-镜像的运行："><a href="#3-2-镜像的运行：" class="headerlink" title="3.2 镜像的运行："></a>3.2 镜像的运行：</h4><ul>
<li>查看镜像id: <code>docker images</code> </li>
<li>运行镜像：<code>docker run -it -v /d:/home image_id /bin/bash</code></li>
</ul>
<h3 id="4-容器快照、镜像的导入-导出"><a href="#4-容器快照、镜像的导入-导出" class="headerlink" title="4. 容器快照、镜像的导入/导出"></a>4. 容器快照、镜像的导入/导出</h3><p>容器快照与镜像的区别：</p>
<ul>
<li>快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也更大。</li>
<li>docker save 保存的是镜像（image），docker export 保存的是容器（container）；</li>
<li>docker load 用来载入镜像包，docker import 用来载入容器包，但两者都会恢复为镜像；</li>
<li>docker load 不能对载入的镜像重命名，而 docker import 可以为镜像指定新名称，如： <code>docker import ubuntu.tar merge_gt/ubuntu:v1(新名称)</code>。</li>
</ul>
<h4 id="4-1-导出-导入容器快照："><a href="#4-1-导出-导入容器快照：" class="headerlink" title="4.1 导出/导入容器快照："></a>4.1 导出/导入容器快照：</h4><p>导出容器快照:</p>
<ul>
<li><code>docker ps</code> 查看正在运行（想要导出）的容器id</li>
<li><code>docker export 容器_id &gt; ubuntu.tar</code></li>
</ul>
<p>导入容器快照:</p>
<ul>
<li>将快照文件导入到merge_gt/ubuntu:v1: <code>docker import ubuntu.tar(绝对路径) merge_gt/ubuntu:v1</code> </li>
<li><code>docker images</code> 查看 <code>merge_gt/ubuntu:v1</code> 镜像id</li>
<li><code>docker run -it -v /d:/home 镜像_id /bin/bash</code></li>
</ul>
<h4 id="4-2-导出-导入镜像："><a href="#4-2-导出-导入镜像：" class="headerlink" title="4.2 导出/导入镜像："></a>4.2 导出/导入镜像：</h4><p>导出镜像：<code>docker save -o rocketmq.tar rocketmq</code> </p>
<ul>
<li>-o：指定保存的镜像的名字；rocketmq.tar：保存到本地的镜像名称；rocketmq：镜像名字或者IMAGE_ID，通过”docker images”查看</li>
</ul>
<p>导入镜像：<code>docker load -i racketmq.tar</code></p>
<h3 id="5-进入后台运行-退出的容器"><a href="#5-进入后台运行-退出的容器" class="headerlink" title="5. 进入后台运行/退出的容器"></a>5. 进入后台运行/退出的容器</h3><p>如何进入退出的容器？</p>
<ul>
<li>查看退出的容器的方法是：<code>docker ps -a</code></li>
<li>重新进入退出的容器：<code>docker start container_ID</code> (start后，容器后台运行)</li>
</ul>
<p>如何进入后台运行的docker？</p>
<ul>
<li><code>docker ps</code> 查看后台运行的docker<br>在使用 -d 参数时，容器启动后会进入后台。此时想要进入容器，可以通过以下指令进入：</li>
<li>docker attach: 使用方法: <code>docker attach container_ID</code>。但是需要注意的是，如果从这个stdin中exit，会导致容器的停止。</li>
<li>docker exec：推荐大家使用 docker exec 命令，因为此退出容器终端，不会导致容器的停止.使用方法：<code>docker exec -it container_ID /bin/bash</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/22/%E9%82%AF%E9%83%B8%E8%B4%AD%E6%88%BF%E7%BC%B4%E7%A8%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/22/%E9%82%AF%E9%83%B8%E8%B4%AD%E6%88%BF%E7%BC%B4%E7%A8%8E/" class="post-title-link" itemprop="url">邯郸购房缴税</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-22 09:08:12" itemprop="dateCreated datePublished" datetime="2020-06-22T09:08:12+08:00">2020-06-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-24 08:57:34" itemprop="dateModified" datetime="2020-06-24T08:57:34+08:00">2020-06-24</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="邯郸"><a href="#邯郸" class="headerlink" title="邯郸:"></a>邯郸:</h2><h4 id="契税"><a href="#契税" class="headerlink" title="契税"></a>契税</h4><p>契税一般是3%，如果是首次购房，可以开具首次购房证明，如果购买的房子面积小于90平米，可以按1%缴纳，如果面积大于90平米小于140平米，可以按照1.5%缴纳，如果大于140平按3%。如果是2套房,一律按3%缴纳.</p>
<h4 id="印花税"><a href="#印花税" class="headerlink" title="印花税"></a>印花税</h4><p>印花税（0.1%,买卖双方各0.05％），印花税是针对合同或者具有合同性质的凭证，产权转移书据，营业账簿，权利、许可证照和经财政部确定征税的其他凭证所收的一类税费。对于购房者而言，印花税的税率是0.05％，即购房者应纳税额为计税价格×0.05％的数值，印花税采取由纳税人自行缴纳完税的方式。</p>
<h4 id="手续费-查到两个-差不多"><a href="#手续费-查到两个-差不多" class="headerlink" title="手续费(查到两个,差不多)"></a>手续费(查到两个,差不多)</h4><p>1.房屋买卖手续费：120平方米以下1000元，120平方米以上3000元，买卖双方各负担一半。</p>
<p>2.二手房交易手续费总额：住宅6元/平米*实际测绘面积，非住宅10元/平米</p>
<h4 id="营业税"><a href="#营业税" class="headerlink" title="营业税"></a>营业税</h4><p>营业税由城市维护建设税、教育费附加、地方教育附加和销售营业税组成，征收税率为5.6％，个人购买超过２年（含２年）的普通住宅对外销售的，免征营业税。</p>
<h4 id="测绘费"><a href="#测绘费" class="headerlink" title="测绘费"></a>测绘费</h4><p>测绘费：1.36元/平米　　总额=1.36元/平米*实际测绘面积(08年4月后新政策房改房测绘费标准：面积75平米以下收200元，75平米以上144平米以下收300元，144平米以上收400元)一般说来房改房都是需要测绘的，商品房如果原产权证上没有济南市房管局的测绘章也是需要测绘的。</p>
<h4 id="登记费"><a href="#登记费" class="headerlink" title="登记费"></a>登记费</h4><p>登记费：(工本费)80元, 共有权证:20元。　</p>
<p>所需材料：　　⑴地税局需要卖方夫妻双方身份证和户口本复印件一套(若卖方夫妻不在同一个户口本上还需提供结婚证复印件一套)、买方身份证复印件一套、网签买卖协议一份、房产证复印件一套(如果卖方配偶已经去世还需要派出所的死亡证明一份)　　⑵房管局需要网签买卖协议一份、房产证原件、新测绘图纸两张，免税证明或完税证明复印件;如省直房改房还需已购公房确认表原件两份和附表一。　　注：房改房过户时需要配偶一起出面签字;若配偶已经去世但使用了其工龄，如果是在房改之后则需要先做继承公证再交易过户;如在房改之前，则应提交派出所开具的死亡证明原件。省直房改房还需填写《已购公房确认表》两份并由单位和省直房改办盖章确认，并提交房改原始票据原件</p>
<h4 id="公证费"><a href="#公证费" class="headerlink" title="公证费"></a>公证费</h4><p>公证处房产收费是按房屋的市场价格来收费的，具体收费标准如下：<br>一：证明土地使用权出让、转让，房屋转让、买卖及股权转让 标的额500000元以下部分，收取比例为0．3％，按比例收费不到200元的，按200元收取，500001元至5000000元部分，收取0．25％，5000001元至10000000元部分， 收取0.2％，10000001元至20000000元部分，收取0.15％，20000001元至50000000元部分，收取0. 1％，50000001元至100000000元部分，收取0．05％，100000001元以上部分，收取0. 01％。</p>
<p>假如120万购买的房屋,需缴纳500000x0.3% + 700000x0.25%=3250元</p>
<h4 id="个人所得税-卖方缴纳"><a href="#个人所得税-卖方缴纳" class="headerlink" title="个人所得税(卖方缴纳)"></a>个人所得税(卖方缴纳)</h4><p>个人所得税在房产交易过程中需要由卖方缴纳，缴纳比例固定，但是也存在个人所得税减免情况。</p>
<p>个人所得税：(税率交易总额1%或两次交易差的20%卖方缴纳)征收条件以家庭为单位出售非唯一住房需缴纳个人房转让所得税。在这里有两个条件①家庭唯一住宅②购买时间超过5年。如果两个条件同时满足可以免交个人所得税;任何一个条件不满足都必须缴纳个人所得税。</p>
<ul>
<li>计算方式1:</li>
</ul>
<p>纳税人(卖方)能在地税系统中查到房屋原值，或能提供房屋原值等费用，个人所得税计算方法为：</p>
<p>个人所得税=(计税价格-房屋原值-原契税-本次交易所缴纳税等合理费用)×20%。</p>
<p>　　举例：如果卖方出卖不满是“满五唯一：的房子，计税价格为100万，原值、原契税以及相关税费合计70万元。那么，卖方需要缴纳的个人所得税为：(100-70)×20%=6万元。</p>
<ul>
<li>计算方式2:</li>
</ul>
<p>纳税人(卖方)不能在地税系统中查到房屋原值，也不能提供房屋原值等费用，个人所得税计算方法为：个人所得税=计税价格×1%。</p>
<p>　　举例：卖家不满五或不唯一的住房出售价为100万售。买家承担税费，全额的1%。那么，卖方需要缴纳的个人所得税为：100×1%=1万元。</p>
<h2 id="五个买房陷阱-只要符合一个-就不能签合同"><a href="#五个买房陷阱-只要符合一个-就不能签合同" class="headerlink" title="五个买房陷阱,只要符合一个,就不能签合同:"></a>五个买房陷阱,只要符合一个,就不能签合同:</h2><p>随着房产市场的火热，业内的竞争越来越激烈。这也使得一些开发商不愿再务实，而是把重心放在了宣传上面。于是市场内出现许多噱头甚至陷阱。而对于经验少的买房者，就容易被这些外在的表象所吸引而中了招。所以下面总结了外行人容易中招的5个买房陷阱，房产经理说买房时只要符合其中一点，房子再好也不要签合同，不然后悔都没处说理。</p>
<h4 id="1、定金陷阱-正确写法应该是订金"><a href="#1、定金陷阱-正确写法应该是订金" class="headerlink" title="1、定金陷阱(正确写法应该是订金)"></a>1、定金陷阱(正确写法应该是订金)</h4><p>我们在买房时有时会要交一部分订金，这样证明自己有意向购买，售楼员也会为我们预留房子。一般来说如果未来一些原因没能购买成功，这个订金也是可以退的。不过有些开发商为了留住客户，将原本的”订金”改为定金，这样日后如果我们没有购买开发商的房子，这笔钱就无法退还给我们。所以在签合同时一定要看好这个定金是否被改写。</p>
<p>￼</p>
<h4 id="2、5证齐备"><a href="#2、5证齐备" class="headerlink" title="2、5证齐备"></a>2、5证齐备</h4><p>房子的5证是买房最基本的保证，如果一个开发商连基本的证件都无法备齐，那么我们日后的利益也会受到影响。一旦出现问题是不受到法律保护的。而且拿不到5证的房产也证明的一些地方是不合格的，不然地方不会不予批准。而这样的房子质量也是很难保证的，日后房子也容易出现各种问题。五证:建设用地规划许可证\国有土地使用证\建设工程规划许可证\建设工程施工许可证\商品房预售许可证.</p>
<h4 id="3、交付时间"><a href="#3、交付时间" class="headerlink" title="3、交付时间"></a>3、交付时间</h4><p>与购买新房的业主来说，在签合同时一定要看好交付时间。因为不少开发商会在这方面做手脚从而延迟交付，所以在签合同时一定要看好上面写的是交房还是验收时间。这两个一定不要搞错了，不然延期交房耽误的只能是我们业主朋友。</p>
<p>￼</p>
<h4 id="4、身份核实"><a href="#4、身份核实" class="headerlink" title="4、身份核实"></a>4、身份核实</h4><p>这一点对于打算买二手房的朋友尤其重要。在买房前一定要了解卖方人是否是真正的房主，或者是夫妇共有。不然就算我们签了合同，后续也会由于家人的纠纷而导致合同失效。另外我们在买房时还要注意，如果是代理人代替签合同也要代理方出具授权的证明。</p>
<p>除此之外在买房时还要了解一下房子的具体情况。由于一些房产是已被查封或抵押的房产，如果不了解就签了合同那么日后吃亏的也只能是自己。</p>
<h4 id="5、联系方式"><a href="#5、联系方式" class="headerlink" title="5、联系方式"></a>5、联系方式</h4><p>现在是信息的年代，我们日常沟通都会用到手机和一些交流软件，所以很多人在填写联系方式时往往忽略了实际的联系地址，等到之后开发商开具发票、合同等文件，如果是以邮寄的形式交付没有地址只能后期自己跑一趟了，也是比较麻烦的。因此在签合同时，无论是电子联系方式还是实际的地址一定要填写详细。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/20/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/20/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-20 20:11:59" itemprop="dateCreated datePublished" datetime="2020-06-20T20:11:59+08:00">2020-06-20</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>357</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/20/%E5%86%9B%E9%98%9F%E6%96%87%E8%81%8C%E5%8E%86%E5%B9%B4%E8%80%83%E8%AF%95%E7%9C%9F%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/20/%E5%86%9B%E9%98%9F%E6%96%87%E8%81%8C%E5%8E%86%E5%B9%B4%E8%80%83%E8%AF%95%E7%9C%9F%E9%A2%98/" class="post-title-link" itemprop="url">军队文职历年考试真题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-20 13:42:28 / 修改时间：20:12:21" itemprop="dateCreated datePublished" datetime="2020-06-20T13:42:28+08:00">2020-06-20</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>84</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="2013年军队文职真题及答案"><a href="#2013年军队文职真题及答案" class="headerlink" title="2013年军队文职真题及答案"></a>2013年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2013年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2014年军队文职真题及答案"><a href="#2014年军队文职真题及答案" class="headerlink" title="2014年军队文职真题及答案"></a>2014年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2014年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2015年军队文职真题及答案"><a href="#2015年军队文职真题及答案" class="headerlink" title="2015年军队文职真题及答案"></a>2015年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2015年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2016年军队文职真题及答案"><a href="#2016年军队文职真题及答案" class="headerlink" title="2016年军队文职真题及答案"></a>2016年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2016年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2018年军队文职真题及答案"><a href="#2018年军队文职真题及答案" class="headerlink" title="2018年军队文职真题及答案"></a>2018年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2018年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2019年军队文职真题及答案"><a href="#2019年军队文职真题及答案" class="headerlink" title="2019年军队文职真题及答案"></a>2019年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2019年军队文职真题及答案解析.pdf" data-height="600px"></div>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/RNN%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/RNN%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/" class="post-title-link" itemprop="url">RNN前向传播、反向传播与并行计算（非常详细）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:56" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:56+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RNN前向传播、反向传播与并行计算-非常详细"><a href="#RNN前向传播、反向传播与并行计算-非常详细" class="headerlink" title="RNN前向传播、反向传播与并行计算(非常详细)"></a>RNN前向传播、反向传播与并行计算(非常详细)</h1><h2 id="1-RNN前向传播"><a href="#1-RNN前向传播" class="headerlink" title="1. RNN前向传播"></a>1. RNN前向传播</h2><p>在介绍RNN之前，首先比较一下RNN与CNN的区别：</p>
<ul>
<li>RNN是一类用于处理序列数据的神经网络，CNN是一类用于处理网格化数据(如一幅图像)的神经网络。</li>
<li>RNN可以扩展到更长的序列，大多数RNN也能处理可变长度的序列。CNN可以很容易地扩展到具有很大宽度和高度的图像，并且可以处理可变大小的图像。</li>
</ul>
<p><img src="./images/RNN-前向.jpg" alt="RNN示意图"><br>RNN的前向传播如图所示，其中$f(x)$代表激活函数，输出的label可以使用one-hot形式。图中所有的$U、W、V、b_1、b_2$全部相同，类似于CNN中的权值共享。CNN通过权值共享可以处理任意大小的图片，RNN通过权值共享，可以处理任意序列长度的语音、句子。</p>
<p>损失函数：</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^{t}||o_i-\hat{o}_i||^2=J_1+J_2+...+J_t（J_i为MSE损失或CE损失）</script><h2 id="2-RNN反向传播"><a href="#2-RNN反向传播" class="headerlink" title="2.RNN反向传播"></a>2.RNN反向传播</h2><p>在介绍RNN反向传播之前，先回顾一下基本神经元的反向传播算法：<br><img src="./images/base.png" alt></p>
<script type="math/tex; mode=display">
\begin{array}{l}\left\{ \begin{matrix}
h=&WX+b\\
S=&f(h)
\end{matrix}\right.
\end{array}</script><p>假设已知损失对$S$的梯度$\frac{\partial J}{\partial S}$:</p>
<script type="math/tex; mode=display">
\begin{array}{l}\left\{ \begin{matrix}
\frac{\partial J}{\partial h}=\frac{\partial J}{\partial S}\frac{d S}{d h}\\
\\
\frac{\partial J}{\partial X}=\frac{\partial J}{\partial h}W^T\\ \\
\frac{\partial J}{\partial W}=X^T\frac{\partial J}{\partial h}\\ \\
\frac{\partial J}{\partial b}=SumCol(\frac{\partial J}{\partial h})

\end{matrix}\right.
\end{array}</script><p>具体推导过程请参考：<a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a></p>
<p>下面介绍RNN的反向传播，如图所示：<br><img src="./images/RNN_bp1.png" alt><br><img src="./images/RNN_bp2.png" alt><br><img src="./images/RNN_bp3.jpg" alt><br>因为共享权重，所以整个RNN网络对$V、W、U$的梯度为:</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial V}=\sum_{i=1}^{t} s_{i}^{T} \frac{\partial J}{\partial o_{i}}; \quad \frac{\partial J}{\partial W}=\sum_{i=1}^{t-1} s_{i}^{T} \frac{\partial J}{\partial h_{i+1}}; \quad \frac{\partial J}{\partial U}=\sum_{i=1}^{t} x_{i}^{T} \frac{\partial J}{\partial h_{i}}</script><h2 id="3-RNN并行加速计算"><a href="#3-RNN并行加速计算" class="headerlink" title="3. RNN并行加速计算"></a>3. RNN并行加速计算</h2><h3 id="3-1-前向并行运算"><a href="#3-1-前向并行运算" class="headerlink" title="3.1 前向并行运算"></a>3.1 <strong>前向并行运算</strong></h3><p>因为RNN为延时网络，网络的每个输入都与前一个时刻的输出有关系，因此，当输入只有一句话时，无法并行计算。当有输入为一个batch时，如何并行计算呢？<br><img src="./images/RNN-并行1.png" alt></p>
<p><img src="./images/RNN-并行2.jpg" alt></p>
<p>也就是说，可以将一个batch的样本在某一个时刻的输入输出并行，加速计算，而不是将一个样本的整个过程并行（因为依赖性无法并行）。</p>
<h3 id="3-2-反向并行计算"><a href="#3-2-反向并行计算" class="headerlink" title="3.2 反向并行计算"></a>3.2 <strong>反向并行计算</strong></h3><p>反向并行运算方式如下图所示：<br><img src="./images/RNN-并行3.jpg" alt></p>
<h2 id="4-双向RNN"><a href="#4-双向RNN" class="headerlink" title="4. 双向RNN"></a>4. 双向RNN</h2><p><img src="./images/RNN-双向.jpg" alt><br>注：图中的$W与\hat{W}$、$U与\hat{U}$、$V与\hat{V}$不同。</p>
<h2 id="5-DeepRNN"><a href="#5-DeepRNN" class="headerlink" title="5. DeepRNN"></a>5. DeepRNN</h2><p><img src="./images/RNN-DeepRnn.png" alt></p>
<p>参考资料：深度之眼</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/CNN%E7%9A%84%E5%89%8D%E5%90%91%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/CNN%E7%9A%84%E5%89%8D%E5%90%91%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" class="post-title-link" itemprop="url">四张图彻底搞懂CNN反向传播算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:11:59" itemprop="dateModified" datetime="2020-06-20T20:11:59+08:00">2020-06-20</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="四张图彻底搞懂CNN反向传播算法"><a href="#四张图彻底搞懂CNN反向传播算法" class="headerlink" title="四张图彻底搞懂CNN反向传播算法"></a>四张图彻底搞懂CNN反向传播算法</h1><p>阅读本文之前，请首先阅读之前讲述的全连接层的反向传播算法详细推导过程： <a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a><br>已经了解反向传播算法请自动忽略。</p>
<h2 id="1-卷积层的反向传播"><a href="#1-卷积层的反向传播" class="headerlink" title="1. 卷积层的反向传播"></a>1. 卷积层的反向传播</h2><p>直接上图：<br><img src="./images/cnn.png" alt><br>假设输入为一张单通道图像$x$，卷积核大小为$2\times 2$，输出为$y$。为了加速计算，首先将$x$按卷积核滑动顺序依次展开，如上图所示。其中，$\hat{x}$中的红色框代表$x$中的红色框展开后的结果，将$x$依次按照此方式展开，可得$\hat{x}$。同理可得$\hat{w}$，然后通过矩阵相乘可得输出$\hat{y}$（$\hat{y}$与$y$等价）。此时，已经将CNN转化为FC，反向传播算法与<a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a> 完全一致，这里不再做详细介绍。当输入图像有N个样本时，应该怎么算呢？</p>
<p>当有$N$个样本，即batch=N时，前向与反向传播方式如下图所示：<br><img src="./images/cnn-batch.png" alt><br>其中，输入图像batch=3,使用2个$2\times 2\times 3$的卷积核，输出两张图像，如图所示。红色框、黄色框代表的是卷积核以及使用该卷积核得到的输出图像$y$。当输入图像为一个batch时，$x、w$的转化方式如上图，首先将输入图像与卷积核分别按单通道图像展开，然后将展开后的矩阵在行方向级联。此时，已经将CNN转化为了FC，反向传播算法与<a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a> 完全一致，这里不再做过多介绍。</p>
<h2 id="2-Average-pooling的反向传播"><a href="#2-Average-pooling的反向传播" class="headerlink" title="2. Average pooling的反向传播"></a>2. Average pooling的反向传播</h2><p><img src="./images/average-pooling.png" alt><br>$\frac{\partial J}{\partial w}$不用求，因为$w$为常数。$\frac{\partial J}{\partial x<em>{ij}}=\Sigma \frac{\partial J}{\partial \hat{x}</em>{ij}}$</p>
<h2 id="3-Max-pooling的反向传播"><a href="#3-Max-pooling的反向传播" class="headerlink" title="3. Max-pooling的反向传播"></a>3. Max-pooling的反向传播</h2><p><img src="./images/max-pooling.png" alt><br>遍历$\hat{x}$的每一行，找出此行最大值的索引$(i,j)$，然后将$\frac{\partial J}{\partial \hat{x}}$中索引为$(i,j)$的值设为$\frac{\partial J}{\partial \hat{y}}$对应行的值，将此行其余列的值设为0，如上图所示红框所示。假设$\hat{x}$中(1,1)处的值是第一行中最大的值，则将$\frac{\partial J}{\partial y<em>{11}}$赋值给$\frac{\partial J}{\partial \hat{x}}$中索引为$(1,1)$的位置。最后计算:$\frac{\partial J}{\partial x</em>{ij}}=\Sigma \frac{\partial J}{\partial \hat{x}_{ij}}$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/LSTM/" class="post-title-link" itemprop="url">LSTM前向传播与反向传播算法推导（非常详细）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="LSTM前向传播与反向传播算法推导（非常详细）"><a href="#LSTM前向传播与反向传播算法推导（非常详细）" class="headerlink" title="LSTM前向传播与反向传播算法推导（非常详细）"></a>LSTM前向传播与反向传播算法推导（非常详细）</h2><h3 id="1-长短期记忆网络LSTM"><a href="#1-长短期记忆网络LSTM" class="headerlink" title="1.长短期记忆网络LSTM"></a>1.长短期记忆网络LSTM</h3><p>LSTM(Long short-term memory)通过刻意的设计来避免长期依赖问题，是一种特殊的RNN。长时间记住信息实际上是 LSTM 的默认行为，而不是需要努力学习的东西！</p>
<p>所有递归神经网络都具有神经网络的链式重复模块。在标准的RNN中，这个重复模块具有非常简单的结构，例如只有单个tanh层，如下图所示。<br><img src="./images/lstm-rnn.jpg" alt><br>LSTM具有同样的结构，但是重复的模块拥有不同的结构，如下图所示。与RNN的单一神经网络层不同，这里有四个网络层，并且以一种非常特殊的方式进行交互。<br><img src="./images/lstm.jpg" alt></p>
<h4 id="1-1-LSTM—遗忘门"><a href="#1-1-LSTM—遗忘门" class="headerlink" title="1.1 LSTM—遗忘门"></a>1.1 LSTM—遗忘门</h4><p><img src="./images/lstm-1.jpg" alt><br>LSTM 的第一步要决定从细胞状态中舍弃哪些信息。这一决定由所谓“遗忘门层”的 S 形网络层做出。它接收 $h<em>{t-1}$ 和 $x_t$，并且对细胞状态 $C</em>{t−1}$ 中的每一个数来说输出值都介于 0 和 1 之间。1 表示“完全接受这个”，0 表示“完全忽略这个”。</p>
<h4 id="1-2-LSTM—输入门"><a href="#1-2-LSTM—输入门" class="headerlink" title="1.2 LSTM—输入门"></a>1.2 LSTM—输入门</h4><p><img src="./images/lstm-2.jpg" alt><br>下一步就是要确定需要在细胞状态中保存哪些新信息。这里分成两部分。第一部分，一个所谓“输入门层”的 S 形网络层确定哪些信息需要更新。第二部分，一个 tanh 形网络层创建一个新的备选值向量—— $\tilde{C}_t$，可以用来添加到细胞状态。在下一步中我们将上面的两部分结合起来，产生对状态的更新。</p>
<h4 id="1-3-LSTM—细胞状态更新"><a href="#1-3-LSTM—细胞状态更新" class="headerlink" title="1.3 LSTM—细胞状态更新"></a>1.3 LSTM—细胞状态更新</h4><p><img src="./images/lstm-3.jpg" alt><br>现在更新旧的细胞状态 $C_{t−1}$ 更新到 $C_t$。先前的步骤已经决定要做什么，我们只需要照做就好。<br>我们对旧的状态乘以 $f_t$，用来忘记我们决定忘记的事。然后我们加上 $i_t\odot\tilde{C}_t$，这是新的候选值，根据我们对每个状态决定的更新值按比例进行缩放。</p>
<h4 id="1-4-LSTM—输出门"><a href="#1-4-LSTM—输出门" class="headerlink" title="1.4 LSTM—输出门"></a>1.4 LSTM—输出门</h4><p><img src="./images/lstm-4.jpg" alt><br>最后，我们需要确定输出值。输出依赖于我们的细胞状态，但会是一个“过滤的”版本。首先我们运行 S 形网络层，用来确定细胞状态中的哪些部分可以输出。然后，我们把细胞状态输入 tanh（把数值调整到 −1 和 1 之间）再和 S 形网络层的输出值相乘，部这样我们就可以输出想要输出的分。</p>
<h3 id="2-LSTM的变种以及前向、反向传播"><a href="#2-LSTM的变种以及前向、反向传播" class="headerlink" title="2. LSTM的变种以及前向、反向传播"></a>2. LSTM的变种以及前向、反向传播</h3><p>目前所描述的还只是一个相当一般化的 LSTM 网络。但并非所有 LSTM 网络都和之前描述的一样。事实上，几乎所有文章都会改进 LSTM 网络得到一个特定版本。差别是次要的，但有必要认识一下这些变种。</p>
<h4 id="2-1-带有”窥视孔连接”的LSTM"><a href="#2-1-带有”窥视孔连接”的LSTM" class="headerlink" title="2.1 带有”窥视孔连接”的LSTM"></a>2.1 带有”窥视孔连接”的LSTM</h4><p>一个流行的 LSTM 变种由 Gers 和 Schmidhuber 提出，在 LSTM 的基础上添加了一个“窥视孔连接”，这意味着我们可以让门网络层输入细胞状态。<br><img src="./images/lstm-5.jpg" alt><br>上图中我们为所有门添加窥视孔，但许多论文只为部分门添加。</p>
<p><strong>前向传播：</strong><br>在t时刻的前向传播公式为：</p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{l}
    {i_t=\sigma(\tilde{i}_t)=\sigma(W_{xi}x_t+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_i)} \\
    {f_t=\sigma(\tilde{f}_t)=\sigma(W_{xf}x_t+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_f) }\\
    {g_t=\tanh(\tilde{g}_t)=\tanh(W_{xg}x_t+W_{hg}h_{t-1}+b_g)} \\
    {o_t=\sigma(\tilde{o}_t)=\sigma(W_{xo}x_t+W_{ho}h_{t-1}+W_{co}c_{t}+b_o) }\\
    {c_t=c_{t-1}\odot f_t+g_t\odot i_t}\\
    {m_t=\tanh(c_t)}\\
    {h_t=o_t\odot m_t}\\
    {y_t=W_{yh}h_t+b_y}

\end{array}\right.</script><p><strong>反向传播：</strong><br>为了更直观的推导反向传播算法，将上图转化为下图：<br><img src="./images/lstm-bp.png" alt><br>对反向传播算法了解不够透彻的，请参考<a href="https://zhuanlan.zhihu.com/p/79657669，这里有详细的推导过程，本文将直接使用https://zhuanlan.zhihu.com/p/79657669的结论。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669，这里有详细的推导过程，本文将直接使用https://zhuanlan.zhihu.com/p/79657669的结论。</a></p>
<p>已知：$\frac{\partial J}{\partial y<em>t},\frac{\partial J}{\partial c</em>{t+1}},\frac{\partial J}{\partial \tilde{o}<em>{t+1}},\frac{\partial J}{\partial \tilde{f}</em>{t+1}},\frac{\partial J}{\partial \tilde{i}<em>{t+1}},\frac{\partial J}{\partial \tilde{g}</em>{t+1}}$,求某个节点梯度时，首先应该找到该节点的输出节点，然后分别计算所有输出节点的梯度乘以输出节点对该节点的梯度，最后相加即可得到该节点的梯度。如计算$\frac{\partial J}{\partial h<em>t}$时，找到$h_t$节点的所有输出节点$y_t、 \tilde{o}</em>{t+1}、\tilde{f}<em>{t+1}、\tilde{i}</em>{t+1}、\tilde{g}<em>{t+1}$，然后分别计算输出节点的梯度(如$\frac{\partial J}{\partial y_t}$)与输出节点对$h_t$的梯度的乘积（如$\frac{\partial J}{\partial y_t}W</em>{yh}^T$），最后相加即可得到节点$h_t$的梯度:</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial h_t}=\frac{\partial J}{\partial y_t}W_{yh}^T+\frac{\partial J}{\partial \tilde{o}_{t+1}}W_{ho}^T+\frac{\partial J}{\partial \tilde{f}_{t+1}}W_{hf}^T+\frac{\partial J}{\partial \tilde{i}_{t+1}}W_{hi}^T+\frac{\partial J}{\partial \tilde{g}_{t+1}}W_{hg}^T</script><p>同理可得t时刻其它节点的梯度：</p>
<script type="math/tex; mode=display">
\left \{\begin{array}{l}
    \frac{\partial J}{\partial h_t}=\frac{\partial J}{\partial y_t}W_{yh}^T+\frac{\partial J}{\partial \tilde{o}_{t+1}}W_{ho}^T+\frac{\partial J}{\partial \tilde{f}_{t+1}}W_{hf}^T+\frac{\partial J}{\partial \tilde{i}_{t+1}}W_{hi}^T+\frac{\partial J}{\partial \tilde{g}_{t+1}}W_{hg}^T \\ \\
    \frac{\partial J}{\partial m_t} = \frac{\partial J}{\partial h_t} \odot o_t \\ \\
    \frac{\partial J}{\partial c_t} = \frac{\partial J}{\partial m_t}\frac{dm_t}{dc_t}+ \frac{\partial J}{\partial c_{t+1}}\odot f_{t+1} +\frac{\partial J}{\partial \tilde{f}_{t+1}}W_{cf}^T+\frac{\partial J}{\partial \tilde{i}_{t+1}}W_{ci}^T \\ \\
    \left. \begin{array}{l}
         \frac{\partial J}{\partial g_t} = \frac{\partial J}{\partial c_t}\odot i_t \\
         \frac{\partial J}{\partial i_t} = \frac{\partial J}{\partial c_t} \odot g_t \\
         \frac{\partial J}{\partial f_t} = \frac{\partial J}{\partial c_t} \odot c_{t-1} \\
         \frac{\partial J}{\partial o_t} = \frac{\partial J}{\partial h_t} \odot m_t
    \end{array} \right \} \Rightarrow \left\{ \begin{array}{l}
        \frac{\partial J}{\partial \tilde{g}_t} = \frac{\partial J}{\partial g_t}(1-g_t^2) \\
        \frac{\partial J}{\partial \tilde{i}_t} = \frac{\partial J}{\partial i_t}i_t(1-i_t) \\
        \frac{\partial J}{\partial \tilde{f}_t} = \frac{\partial J}{\partial f_t}f_t(1-f_t) \\
        \frac{\partial J}{\partial \tilde{o}_t} = \frac{\partial J}{\partial o_t}i_t(1-o_t) \\
    \end{array}\right. \\ \\
    \frac{\partial J}{\partial x_t} = \frac{\partial J}{\partial \tilde{o}_t}W_{xo}^T+\frac{\partial J}{\partial \tilde{f}_t}W_{xf}^T+ \frac{\partial J}{\partial \tilde{i}_t}W_{xi}^T+\frac{\partial J}{\partial \tilde{g}_t}W_{xg}^T\\
\end{array}\right.</script><p>对参数的梯度：</p>
<script type="math/tex; mode=display">
\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{ho}} = h_t^T\frac{\partial J}{\partial \tilde{o}_{t+1}} \\
    \frac{\partial J}{\partial W_{hf}} = h_t^T\frac{\partial J}{\partial \tilde{f}_{t+1}} \\
    \frac{\partial J}{\partial W_{hi}} = h_t^T\frac{\partial J}{\partial \tilde{i}_{t+1}} \\
    \frac{\partial J}{\partial W_{hg}} = h_t^T\frac{\partial J}{\partial \tilde{g}_{t+1}}
\end{array} \right. 

\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{yh}} = h_t^T\frac{\partial J}{\partial y_t} \\
    \frac{\partial J}{\partial W_{cf}} = c_t^T\frac{\partial J}{\partial \tilde{f}_{t+1}} \\
    \frac{\partial J}{\partial W_{ci}} = c_t^T\frac{\partial J}{\partial \tilde{i}_{t+1}} \\
    \frac{\partial J}{\partial W_{co}} = c_t^T\frac{\partial J}{\partial \tilde{o}_{t}} 
\end{array} \right. 

\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{xo}} = x_t^T\frac{\partial J}{\partial \tilde{o}_{t}} \\
    \frac{\partial J}{\partial W_{xf}} = x_t^T\frac{\partial J}{\partial \tilde{f}_{t}} \\
    \frac{\partial J}{\partial W_{xi}} = x_t^T\frac{\partial J}{\partial \tilde{i}_{t}} \\
    \frac{\partial J}{\partial W_{xg}} = x_t^T\frac{\partial J}{\partial \tilde{g}_{t}} \\
\end{array} \right.</script><h4 id="2-2-GRU"><a href="#2-2-GRU" class="headerlink" title="2.2 GRU"></a>2.2 GRU</h4><p>一个更有意思的 LSTM 变种称为 Gated Recurrent Unit（GRU），由 Cho 等人提出。LSTM通过三个门函数输入门、遗忘门和输出门分别控制输入值、记忆值和输出值。而GRU中只有两个门：更新门$z_t$和重置门$r_t$，如下图所示。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多；重置门控制前一时刻状态有多少信息被写入到当前的候选集$\tilde{h}_t$上，重置门越小，前一状态的信息被写入的越少。这样做使得 GRU 比标准的 LSTM 模型更简单，因此正在变得流行起来。<br><img src="./images/lstm-7.jpg" alt><br>为了更加直观的推导反向传播公式，将上图转化为如下形式：<br><img src="./images/lstm-gru.png" alt><br><strong>GRU的前向传播：</strong> 在t时刻的前向传播公式为：</p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{l}
    {r_t=\sigma(\tilde{r}_t)=\sigma(W_{xr}x_t+W_{hr}h_{t-1}+b_r)} \\
    {z_t=\sigma(\tilde{z}_t)=\sigma(W_{xz}x_t+W_{hz}h_{t-1}+b_z) }\\
    {s_t=\tanh(\tilde{s}_t)}=\tanh[W_{xs}x_t+(h_{t-1}\odot r_t)W_{hs}+b_s]  \\
    {h_t=z_t\odot s_t + h_{t-1}\odot (1-z_t)}\\
    {y_t=W_{yh}h_t+b_y}
\end{array}\right.</script><p><strong>GRU的反向传播：</strong><br>t时刻其它节点的梯度:</p>
<script type="math/tex; mode=display">
\left\{ \begin{array}{l}
    \frac{\partial J}{\partial h_t}=\frac{\partial J}{\partial y_t}W_{hy}^T+\frac{\partial J}{\partial \tilde{r}_{t+1}}W_{hr}^T+ \frac{\partial J}{\partial \tilde{z}_{t+1}}W_{hz}^T+ \frac{\partial J}{\partial \tilde{s}_{t+1}}W_{hs}^T \odot  r_t + \frac{\partial J}{\partial \tilde{h}_{t+1}}\odot(1-z_t)\\
\left.\begin{array}{l}
    \frac{\partial J}{\partial s_t}=\frac{\partial J}{\partial h_t}\odot z_t\\
    \frac{\partial J}{\partial z_t}=\frac{\partial J}{\partial h_t}\odot s_t + \frac{\partial J}{\partial h_t}\odot (-h_{t-1})  \\
    \frac{\partial J}{\partial r_t}=\frac{\partial J}{\partial \tilde{s}_t}W_{hs}^T\odot h_{t-1} \\
\end{array}\right\} \Rightarrow
\left\{ \begin{array}{l}
    \frac{\partial J}{\partial \tilde{s}_t}=\frac{\partial J}{\partial s_t}(1-s_t^2)\\
    \frac{\partial J}{\partial \tilde{z}_t}=\frac{\partial J}{\partial z_t}z_t(1-z_t)  \\
    \frac{\partial J}{\partial \tilde{r}_t}=\frac{\partial J}{\partial r_t}r_t(1-r_t) \\
\end{array}\right. 
\\
    \frac{\partial J}{\partial x_t} = \frac{\partial J}{\partial \tilde{r}_t}W_{xr}^T+\frac{\partial J}{\partial \tilde{z}_t}W_{xz}^T+ \frac{\partial J}{\partial \tilde{s}_t}W_{xs}^T\\
\end{array}\right.</script><p>对参数的梯度：</p>
<script type="math/tex; mode=display">
\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{hy}} = h_t^T\frac{\partial J}{\partial y_t} \\
    \frac{\partial J}{\partial W_{hs}} = (h_{t-1}\odot r_t)^T\frac{\partial J}{\partial \tilde{s}_{t}} \\
    \frac{\partial J}{\partial W_{hz}} = h_{t-1}^T\frac{\partial J}{\partial \tilde{z}_{t}} \\
    \frac{\partial J}{\partial W_{hr}} = h_{t-1}^T\frac{\partial J}{\partial \tilde{r}_{t}}
\end{array} \right. 
\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{xs}} = x_t^T\frac{\partial J}{\partial \tilde{s}_{t}} \\
    \frac{\partial J}{\partial W_{xz}} = x_t^T\frac{\partial J}{\partial \tilde{f}_{z}} \\
    \frac{\partial J}{\partial W_{xr}} = x_t^T\frac{\partial J}{\partial \tilde{r}_{t}} 
\end{array} \right.</script><h4 id="2-3-结合遗忘门与输入门的LSTM"><a href="#2-3-结合遗忘门与输入门的LSTM" class="headerlink" title="2.3 结合遗忘门与输入门的LSTM"></a>2.3 结合遗忘门与输入门的LSTM</h4><p>另一个变种把遗忘和输入门结合起来。同时确定要遗忘的信息和要添加的新信息，而不再是分开确定。当输入的时候才会遗忘，当遗忘旧信息的时候才会输入新数据。<br><img src="./images/lstm-6.jpg" alt><br>前向与反向算法与上述变种相同，这里不再做过多推导。</p>
<blockquote>
<p>参考资料：<a href="https://www.cnblogs.com/xuruilong100/p/8506949.html" target="_blank" rel="noopener">https://www.cnblogs.com/xuruilong100/p/8506949.html</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/LDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/LDA/" class="post-title-link" itemprop="url">线性判别分析LDA原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-28 15:53:17" itemprop="dateModified" datetime="2020-06-28T15:53:17+08:00">2020-06-28</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="线性判别分析LDA原理"><a href="#线性判别分析LDA原理" class="headerlink" title="线性判别分析LDA原理"></a>线性判别分析LDA原理</h2><p>线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了解下它的算法原理。</p>
<h3 id="1-LDA的思想"><a href="#1-LDA的思想" class="headerlink" title="1. LDA的思想"></a>1. LDA的思想</h3><p>LDA的思想是：<font color="#FF0000"> <strong>最大化类间均值，最小化类内方差</strong></font>。意思就是将数据投影在低维度上，并且投影后同种类别数据的投影点尽可能的接近，不同类别数据的投影点的中心点尽可能的远。</p>
<p>我们先看看最简单的情况。假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。<br><img src="/2020/06/18/LDA/LDA.jpg" alt></p>
<p>上图提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。</p>
<h3 id="2-瑞利商-Rayleigh-quotient-与广义瑞利商-genralized-Rayleigh-quotient"><a href="#2-瑞利商-Rayleigh-quotient-与广义瑞利商-genralized-Rayleigh-quotient" class="headerlink" title="2. 瑞利商(Rayleigh quotient)与广义瑞利商(genralized Rayleigh quotient)"></a>2. 瑞利商(Rayleigh quotient)与广义瑞利商(genralized Rayleigh quotient)</h3><p>首先来看瑞利商的定义。瑞利商是指这样的函数$R(A,x)$:</p>
<script type="math/tex; mode=display">
R(A, x)=\frac{x^{H} A x}{x^{H} x}</script><p>其中，$x$是非零向量，而$A$为$n\times n$的Hermitan矩阵。所谓的Hermitan矩阵就是满足<strong>共轭转置矩阵</strong>和自己相等的矩阵，即$A^H=A$，例如，$A=\left(\begin{array}{cc}{1} &amp; {2+i} \ {2-i} &amp; {1}\end{array}\right)$的共轭转置等于其本身。当$A$为实矩阵时，如果满足$A^T=A$，则$A$为Hermitan矩阵。</p>
<p>瑞利商$R(A,x)$有一个非常重要的性质，即<font color="#FF0000"><strong>它的最大值等于矩阵$A$最大的特征值，而最小值等于矩阵$A$的最小的特征值</strong></font>，也就是满足</p>
<font color="#FF0000">
$$
\lambda_{\min } \leq \frac{x^{H} A x}{x^{H} x} \leq \lambda_{\max }
$$
</font>

<p>当向量$x$是标准正交基，即满足$x^{H} x=1$时，瑞利商退化为$R(A, x)=x^{H} A x$。</p>
<p>下面看一下广义瑞利商。广义瑞利商是指这样的函数$R(A,B,x)$:</p>
<script type="math/tex; mode=display">
R(A,B, x)=\frac{x^{H} A x}{x^{H} B x}</script><p>其中x为非零向量，而$A,B$为$n\times n$的Hermitan矩阵。$B$为正定矩阵。它的最大值和最小值是什么呢？其实我们只要通过将其通过标准化就可以转化为瑞利商的格式。令$x=B^{-1 / 2} x^{\prime}$，则分母转化为：</p>
<script type="math/tex; mode=display">
x^{H} B x=x^{\prime H}\left(B^{-1 / 2}\right)^{H} B B^{-1 / 2} x^{\prime}=x^{\prime H} B^{-1 / 2} B B^{-1 / 2} x^{\prime}=x^{\prime H} x^{\prime}</script><p>而分子转化为：</p>
<script type="math/tex; mode=display">
x^{H} A x=x^{\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\prime}</script><p>此时我们的$R(A,B,x)$转化为$R(A,B,x’)$:</p>
<script type="math/tex; mode=display">
R\left(A, B, x^{\prime}\right)=\frac{x^{\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\prime}}{x^{\prime H} x^{\prime}}</script><p>利用前面的瑞利商的性质，我们可以很快的知道，$R(A,B,x’)$的最大值为矩阵$B^{-1 / 2} A B^{-1 / 2}$的最大特征值，或者说矩阵$B^{−1}A$的最大特征值，而最小值为矩阵$B^{−1}A$的最小特征值。</p>
<h3 id="2-LDA的原理及推导过程"><a href="#2-LDA的原理及推导过程" class="headerlink" title="2. LDA的原理及推导过程"></a>2. LDA的原理及推导过程</h3><p>假设样本共有K类，每一个类的样本的个数分别为$N<em>1,N_2,…,N_k$ \<br>$x_1^1,x_1^2,…,x_1^{N_1}$对应第1类\<br>$x_2^1,x_2^2,…,x_2^{N_2}$对应第2类\<br>$x_k^1,x_k^2,…,x_k^{N_k}$对应第K类，其中每个样本$x_i^j$均为$n$维向量 \<br>设$\tilde{x}</em>{i}^{j}$为$x<em>i^j$变化后的样本，则$\tilde{x}</em>{i}^{j}=<x, u>u=|x||u|cos\theta \cdot u =\left(x^{T} u\right) u$</x,></p>
<p>此处设$u$为单位向量，即$u^Tu=1$.</p>
<p>假设第K类样本的数据集为$D<em>k$，变化后的样本的均值向量为：$\tilde{\mathrm{m}}=\frac{\sum</em>{\tilde{x}\in D_k}\tilde{x}}{N_k}$，那么，第K类样本的方差为$\frac{S_k}{N_k}$，其中：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{S}_{\mathrm{k}} &=\sum_{\tilde{x} \in D_{k}}(\tilde{x}-\tilde{\mathrm{m}})^{T}(\tilde{x}-\tilde{\mathrm{m}})\\ 
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right) u-\left(m^{T} u\right) u\right]^T\left[\left(x^{T} u\right) u -\left(m^{T} u\right) u\right] \\
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right) u^{T}-\left(m^{T} u\right) u^{T}\right]\left[\left(x^{T} u\right) u -\left(m^{T} u\right) u\right]  \quad (x^Tu，m^Tu为实数,转置仍是本身)\\ 
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right)^{2} u^{T} u-2\left(x^{T} u\right)\left(m^{T} u\right) u^{T} u+\left(m^{T} u\right)^{2} u^{T} u\right] \\ 
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right)^{2}-2\left(x^{T} u\right)\left(m^{T} u\right)+\left(m^{T} u\right)^{2}\right] \end{aligned}</script><p>第K类样本的方差：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{S_{k}}{N_{k}} &=\frac{\sum_{x \in D_{k}}\left(x^{T} u\right)^{2}}{N_{k}}-2 \frac{\sum_{x \in D_{k}} x^{T}\left(u m^{T} u\right)}{N_{k}}+\frac{\sum_{x\in D_{k}}\left(m^{T} u\right)^{2}}{N_{k}} \\ 
&=\frac{\sum_{x\in D_k} u^{T} x x^{T} u}{N_{k}}-2 \frac{\sum_{x\in D_k} x^{T}}{N_{k}} u m^{T} u+\left(m^{T} u\right)^{2}\quad (注：\frac{\sum_{x\in D_{k}}\left(m^{T} u\right)^{2}}{N_{k}}=(m^Tu)^2。因为m^Tu与x无关)\\ 
&=u^{T} \frac{\sum_{x\in D_k} x x^{T}}{N_{k}} u-\left(m^{T} u\right)^{2} \quad (注：\frac{\sum_{x \in D_k}x^T}{N_k}=m^T)\\ 
&=u^{T} \frac{\sum_{x\in D_k} x x^{T}}{N_{k}} u-u^{T} m m^{T} u \\ 
&=u^{T}\left(\frac{\sum_{x\in D_k} x x^{T}}{N_{k}}-m m^{T}\right) u \end{aligned}</script><p>各个类别的样本方差之和：</p>
<script type="math/tex; mode=display">
\begin{aligned} \sum_{k}\frac{S_k}{N_k} &=\sum_{k=1}^{K}u^{T}\left(\frac{\sum_{x \in D_k} x x^{T}}{N_{k}}-m_k m_k^{T}\right) u \\ &=u^T\sum_{k=1}^{K}(\frac{\sum_{x \in D_k}xx^T}{N_k}-m_km_k^T)u\\&=u^TS_wu
\end{aligned}</script><p>其中，$S<em>w=\sum</em>{k=1}^{K}(\frac{\sum_{x \in D_k}xx^T}{N_k}-m_km_k^T)$，$S_w$一般被称为类内散度矩阵</p>
<p>不同类别$i,j$之间的中心距离：</p>
<script type="math/tex; mode=display">
\begin{aligned} S_{i j} &=\left(\tilde{m}_{i}-\tilde{m}_{j}\right)^{T}\left(\tilde{m}_{i}-\tilde{m}_{j}\right) \\
&=[(u^Tm_i)u-(u^Tm_j)u]^T[(m_i^Tu)u-(m_j^Tu)u] \\
&=[(u^Tm_i)u^T-(u^Tm_j)u^T][u(m_i^Tu)-u(m_j^Tu)] \\
&=u^T(m_i-m_j)u^Tu(m_i^T-m_j^T)u \\
&=u^{T}\left(m_{i}-m_{j}\right)\left(m_{i}-m_{j}\right)^{T} u \end{aligned}</script><p>所有类别之间的距离之和为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \sum_{i, j \atop i \neq j} s_{i j} &= u^T\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]u \\
    &=u^TS_bu
\end{aligned}</script><p>其中，$ S<em>b=\sum</em>{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T] $，$ S_b $一般被称为类间散度矩阵。</p>
<p>在已知条件下，$S_w,S_b$均可求出，LDA算法的目标是“<strong>类间距离尽可能大，类内方差尽可能小</strong>”，即最大化$u^TS_bu$,最小化$u^TS_wu$.</p>
<p>令$ J(u)=\frac{u^{T} S<em>{b} u}{u^{T} S</em>{w} u} $,则目标函数为：</p>
<script type="math/tex; mode=display">
\max J(u)</script><p>为了使所求最大，可假设$u^TS_wu=1$，则问题转化为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\max u^{T} S_{b} u} \\
s.t. \quad u^TS_wu=1 
\end{array}</script><script type="math/tex; mode=display">
\begin{aligned}
 L(u, \lambda)&=u^{T} S_{b} u+\lambda\left(1-u^{T} S_{w} u\right) \\
 \frac{\partial L}{\partial u} &= S_bu+S_b^Tu-\lambda S_wu-\lambda S_w^Tu \\ &=2(S_bu-\lambda S_wu)\quad (因为S_b，S_w为对称矩阵)\\ &=0 \Rightarrow S_{b} u=\lambda S_{w} u \end{aligned}</script><blockquote>
<p>证明$S_b$为对称矩阵：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    S_b^T &= (\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T])^T\\
    &=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]^T \\
    &=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T] \\
    &=S_b
\end{aligned}</script><p>所以，$S_b$为对称矩阵，同理可证明$S_w$为对称矩阵。</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{array}{l}
S_{w}^{-1}{S_{b}  u=\lambda u}
\end{array}</script><p>计算矩阵$S_w^{-1}S_b$的最大的$d$个特征值和对应的$d$个特征向量$(w_1,w_2,…,w_d)$，得到投影矩阵$W=(w_1,w_2,…,w_d)$。</p>
<blockquote>
<p>注意: (1)选取特征值时，如果一些特征值明显大于其他的特征值，则取这些取值较大的特征值，因为它们包含更多的数据分布的信息。相反，如果一些特征值接近于0，我们将这些特征值舍去。<br>(2)由于$W$是一个利用了样本类别得到的投影矩阵，因此它能够降维到的维度d的最大值为$K-1$。为什么不是$K$呢？因为$S_b$中每个$m_i-m_j$的秩均为1(因为$m_i$为1维向量).</p>
<p>由$R(AB)\leq \min(R(A), R(B))$知，</p>
<script type="math/tex; mode=display">R((m_i-m_j)(m_i-m_j)^T)=1</script><p>又 $\because$</p>
<script type="math/tex; mode=display">S_b=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T], \\R(A+B)\leq R(A)+R(B)</script><script type="math/tex; mode=display">\therefore R(S_b)\leq K-1 \Rightarrow R(S_w^{-1}S_b)\leq K-1</script><p>$\therefore \lambda =0$对应的特征向量至少有$(K-(K-1))=1$个，也就是说$d$最大为$K-1$. </p>
</blockquote>
<h3 id="3-LDA算法流程"><a href="#3-LDA算法流程" class="headerlink" title="3. LDA算法流程"></a>3. LDA算法流程</h3><p>输入：数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$,其中，任意样本$x_i$为$n$维向量，$y_i\in {C_1,C_2,…,C_k}$,降维到的维度为$d$.</p>
<p>输出：降维后的数据集$D’$.</p>
<ol>
<li>计算类内散度矩阵$S_w$</li>
<li>计算类间散度矩阵$S_b$</li>
<li>计算矩阵$S_w^{-1}S_b$</li>
<li>计算矩阵$S_w^{-1}S_b$的特征值与特征向量，按从小到大的顺序选取前$d$个特征值和对应的$d$个特征向量$(w_1,w_2,…,w_d)$，得到投影矩阵$W$.</li>
<li>对样本集中的每一个样本特征$x_i$，转化为新的样本$z_i=W^Tx_i$</li>
<li>得到输出样本集$D’={(z_1,y_1),(z_2,y_2),…,(z_m,y_m)}$<h3 id="4-LDA与PCA对比"><a href="#4-LDA与PCA对比" class="headerlink" title="4. LDA与PCA对比"></a>4. LDA与PCA对比</h3>LDA与PCA都可用于降维，因此有很多相同的地方，也有很多不同的地方</li>
</ol>
<ul>
<li><p><strong>相同点：</strong></p>
<ul>
<li>两者均可用于数据降维</li>
<li>两者在降维时均使用了矩阵特征分解的思想</li>
<li>两者都假设数据符合高斯分布</li>
</ul>
</li>
<li><p><strong>不同点：</strong></p>
<ul>
<li>LDA是有监督的降维方法，而PCA是无监督降维方法</li>
<li>当总共有K个类别时，LDA最多降到K-1维，而PCA没有这个限制</li>
<li>LDA除了用于降维，还可以用于分类</li>
<li>LCA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优（如下图的左图）。当然，某些数据分布下PCA比LDA降维较优（如下图的右图）。<table align="center">
<tr>
<td><center><img height="200" src="/2020/06/18/LDA/LDA-2.jpg"></center>图2</td>
<td><center><img height="200" src="/2020/06/18/LDA/LDA-3.jpg"></center>图3</td>
</tr>
</table>

</li>
</ul>
</li>
</ul>
<h3 id="5-LDA算法小结"><a href="#5-LDA算法小结" class="headerlink" title="5. LDA算法小结"></a>5. LDA算法小结</h3><p>LDA算法既可以用来降维，也可以用来分来，但是目前来说，LDA主要用于降维，在进行与图像识别相关的数据分析时，LDA是一个有力的工具。下面总结一下LDA算法的优缺点。</p>
<ul>
<li><strong>LDA算法的优点</strong><ul>
<li>在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习无法使用先验知识；</li>
<li>LDA在样本分类信息依赖均值而不是方差的时候，比PCA算法较优。</li>
</ul>
</li>
<li><strong>LDA算法的缺点</strong><ul>
<li>LDA与PCA均不适合对非高斯分布样本进行降维</li>
<li>LDA降维算法最多降到类别数K-1的维度，当降维的维度大于K-1时，则不能使用LDA。当然目前有一些改进的LDA算法可以绕过这个问题</li>
<li>LDA在样本分类信息依赖方差而非均值的时候，降维效果不好</li>
<li>LDA可能过度拟合数据<blockquote>
<p>参考：<a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6244265.html</a></p>
</blockquote>
</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/" class="post-title-link" itemprop="url">SVD奇异值分解逐步推导</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="SVD奇异值分解逐步推导"><a href="#SVD奇异值分解逐步推导" class="headerlink" title="SVD奇异值分解逐步推导"></a>SVD奇异值分解逐步推导</h2><h3 id="1-回顾特征值和特征向量"><a href="#1-回顾特征值和特征向量" class="headerlink" title="1. 回顾特征值和特征向量"></a>1. 回顾特征值和特征向量</h3><p>首先回顾下特征值和特征向量的定义：</p>
<script type="math/tex; mode=display">
Ax=\lambda x</script><p>其中，A是一个$n\times n$的矩阵，$x$是一个$n$维向量，则$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$对应的特征向量。</p>
<p>求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$\lambda _1\leq \lambda_2\leq … \leq\lambda_n$，以及这$n$个特征值所对应的特征向量$w_1, w_2, …, w_n$,那么矩阵A就可以用以下的特征分解表示：</p>
<script type="math/tex; mode=display">
W^{-1}AW=\Lambda \Leftrightarrow A=W\Lambda W^{-1}</script><p>其中，$W$是这$n$个特征向量所组成的$n\times n$维矩阵，而$\Lambda$是将这$n$个特征值作为主对角线的$n\times n$维矩阵。一般情况下，我们会把$W$的这$n$个特征向量标准化，即满足$||w_i||_2=1$，或者$w_i^Tw_i=1$,此时$W$的$n$个特征向量为标准正交基，满足$W^TW=I$，即$W^T=W^{-1}$,也就是说$W$为酉矩阵。这样我们的特征分解表达式可以写成：</p>
<script type="math/tex; mode=display">
A=W\Lambda W^T</script><blockquote>
<p>题外延伸———矩阵压缩：</p>
<p>设$W=(w_1, w_2, w_3,…,w_n)$，则:</p>
<script type="math/tex; mode=display">W^T=\begin{pmatrix}
 w_1^T\\ 
 w_2^T\\ 
 w_3^T\\ 
... \\
w_n^T
\end{pmatrix}</script><p>那么:</p>
<script type="math/tex; mode=display">
A=(w_1, w_2, w_3,...,w_n)\begin{pmatrix}
    \lambda_1&&&\\
    &\lambda_2&&\\
    &&\lambda_3&\\
    &&&...\\
    &&&&\lambda_n
\end{pmatrix}\begin{pmatrix}
 w_1^T\\ 
 w_2^T\\ 
 w_3^T\\ 
... \\
w_n^T
\end{pmatrix}
\\
=\lambda_1w_1w_1^T+\lambda_2w_2w_2^T+\lambda_3w_3w_3^T+...+\lambda_nw_nw_n^T</script><p>假设A为$n\times n$维矩阵，如果正常表示矩阵A共需使用$n^2$个元素，如果将取得的特征值$\lambda_1,\lambda_2,\lambda_3,…,\lambda_n$按从大到小排序，即$\lambda_1\geq\lambda_2\geq\lambda_3\geq…\geq\lambda_n$，则将A的压缩表示为$\lambda_1w_1w_1^T$，即只需要$n+1$个元素。</p>
</blockquote>
<p>注意到要进行特征分解，矩阵A必须为方阵。</p>
<p>那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。</p>
<h3 id="2-SVD推导"><a href="#2-SVD推导" class="headerlink" title="2. SVD推导"></a>2. SVD推导</h3><h4 id="Step1：矩阵分解"><a href="#Step1：矩阵分解" class="headerlink" title="Step1：矩阵分解"></a>Step1：矩阵分解</h4><p>假如$A$为$m\times n$维矩阵，则$A^TA$为对称正定矩阵。</p>
<blockquote>
<p>证明：1)对称性：$(A^TA)^T=A^TA\Rightarrow 对称性$<br>2)正定性： $x^TA^TAx=(Ax)^T(Ax)\geq 0\Rightarrow正定性$</p>
</blockquote>
<p>对于矩阵A，有$(A^TA)v_i=\lambda _iv_i$，其中$\lambda_i$为特征值，$v_i$为特征向量。假定$(v_i, v_j)$是一组正交基，那么有$v_i^T\cdot v_j=0$，那么：</p>
<script type="math/tex; mode=display">
(Av_i, Av_j)=(Av_i)^T\cdot Av_j=v_i^TA^TAv_j=v_i^T\lambda_jv_j=\lambda_jv_i^Tv_j=0</script><p>因此，$Av_i,Av_j$也是一组正交基，根据上述公式可以推导出$(Av_i, Av_i)=\lambda_iv_i^Tv_i=\lambda_i$,从而可以得到：</p>
<script type="math/tex; mode=display">
|Av_i|^2=\lambda_i</script><script type="math/tex; mode=display">
|Av_i|=\sqrt{\lambda_i}</script><p>根据上述公式，有$\frac{Av_i}{|Av_i|}=\frac{1}{\sqrt{\lambda_i}}Av_i$，令$\frac{1}{\sqrt{\lambda_i}}Av_i=u_i$，可得：</p>
<script type="math/tex; mode=display">
Av_i=\sqrt{\lambda_i}u_i=\delta_i u_i</script><p>其中，$\delta_i=\sqrt{\lambda_i}$，进一步推导：</p>
<script type="math/tex; mode=display">
AV=A(v_1,v_2,...,v_n)=(Av_1,Av_2,...,Av_n)=(\delta_1u_1,\delta_2u_2,...,\delta_nu_n)=U\Sigma</script><p>从而得出：</p>
<script type="math/tex; mode=display">A=U\Sigma V^T</script><h4 id="Step2-矩阵计算"><a href="#Step2-矩阵计算" class="headerlink" title="Step2:矩阵计算"></a>Step2:矩阵计算</h4><p>得到矩阵A的表示后，我们应该如何计算向量$U$和$V$呢？继续往下面分析：</p>
<p>首先计算出A的转置$A^T$：$A^T=V\Sigma ^TU^T$</p>
<script type="math/tex; mode=display">
A^TA=V\Sigma^TU^TU\Sigma V^T=V\Sigma^2V^T</script><p>利用上式可以得到，$A^TAv_i=\lambda_iv_i$，只需要求出$A^TA$的特征向量即可得到$V$.</p>
<p>同理可得$AA^T$的值：</p>
<script type="math/tex; mode=display">
AA^T=U\Sigma V^TV\Sigma^TU^T=U\Sigma^2U^T</script><p>可以得到$AA^Tu_i=\lambda_iu_i$，只需要求出$AA^T$的特征向量即可得到$U$.</p>
<blockquote>
<p>题外延伸——-矩阵(图像)压缩：</p>
<p>一个$m\times n$的矩阵A经SVD分解后，可以写成如下形式：</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}\Sigma V^T_{n\times n}=(u_1,u_2,...,u_m)\begin{pmatrix}
    \lambda_1^{\frac{1}{2}}&&\\
    &\lambda_2^{\frac{1}{2}}&\\
    &&...
\end{pmatrix}\begin{pmatrix}
    v_1^T\\
    v_2^T\\
    ...\\
    v_n^T
\end{pmatrix}\\
=\lambda_1^{\frac{1}{2}}u_1v_1^T+\lambda_2^{\frac{1}{2}}u_2v_2^T+...</script><p>假设A为$m\times n$维矩阵，在没有压缩时表示矩阵A共需要$m\times n$个元素。如果将取得的特征值按从大到小排序，即$\lambda<em>1\geq\lambda_2\geq\lambda_3\geq…\geq\lambda</em>{min{m,n}}$，则A的压缩最小压缩表示为$\lambda_1^{\frac{1}{2}}u_1v_1^T$，即需要$m+n+1$个元素。<br>当压缩储存量为$(m+n+1)\times k$时，误差为</p>
<script type="math/tex; mode=display">
error=\frac{(m+n)\times\sum_{i=1}^{k}\lambda_i}{(m+n)\times\sum_{i=1}^{min(m,n)}\lambda_i}=\frac{\sum_{i=1}^{k}\lambda_i}{\sum_{i=1}^{min(m,n)}\lambda_i}</script></blockquote>
<h3 id="例题讲解"><a href="#例题讲解" class="headerlink" title="例题讲解"></a>例题讲解</h3><p>我们举一个简单的例子讲解矩阵时如何进行奇异值分解的。定义矩阵A为：</p>
<script type="math/tex; mode=display">
A=\begin{pmatrix}
0 &1 \\
1&1\\
1&0  
\end{pmatrix}</script><p>首先求出$A^TA、AA^T$：</p>
<script type="math/tex; mode=display">
A^TA=\begin{pmatrix}
    0&1&1\\
    1&1&0
\end{pmatrix}\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}=\begin{pmatrix}
    2&1\\
    1&2
\end{pmatrix}</script><script type="math/tex; mode=display">
AA^T=\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}\begin{pmatrix}
    0&1&1\\
    1&1&0
\end{pmatrix}=\begin{pmatrix}
    1&1&0\\
    1&2&1\\
    0&1&1
\end{pmatrix}</script><p>进而求出$A^TA$的特征值和特征向量：</p>
<script type="math/tex; mode=display">
\lambda_1=3;v_1=\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix};
\lambda_2=1;v_2=\begin{pmatrix}
    -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix}</script><p>接着求出$AA^T$的特征值和特征向量：</p>
<script type="math/tex; mode=display">
\lambda_1=3;u_1=\begin{pmatrix}
    \frac{1}{\sqrt{6}}\\
    \frac{2}{\sqrt{6}}\\
    \frac{1}{\sqrt{6}}
\end{pmatrix};
\lambda_2=1;u_2=\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    0\\
    -\frac{1}{\sqrt{2}}
\end{pmatrix};\lambda_3=0;u_3=\begin{pmatrix}
    \frac{1}{\sqrt{3}}\\
    -\frac{1}{\sqrt{3}}\\
    \frac{1}{\sqrt{3}}
\end{pmatrix}</script><p>利用$Av_i=\delta_iu_i,i=1,2$求奇异值：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix}=\delta_1\begin{pmatrix}
    \frac{1}{\sqrt{6}}\\
    \frac{2}{\sqrt{6}}\\
    \frac{1}{\sqrt{6}}
\end{pmatrix} \Rightarrow\delta_1=\sqrt{3}</script><script type="math/tex; mode=display">
\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}\begin{pmatrix}
    -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix}=\delta_2\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    0\\
    -\frac{1}{\sqrt{2}}
\end{pmatrix} \Rightarrow\delta_2=1</script><p>也可以用$\delta_i=\sqrt{\lambda_i}$直接求出奇异值为$\sqrt{3}$和$1$.</p>
<p>最终得到矩阵A的奇异值分解为：</p>
<script type="math/tex; mode=display">
A=U\Sigma V^T=\begin{pmatrix}
    \frac{1}{\sqrt{6}}  &\frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{3}} \\
    \frac{2}{\sqrt{6}}  &0  & -\frac{1}{\sqrt{3}} \\
    \frac{1}{\sqrt{6}}  &-\frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{3}} 
\end{pmatrix}\begin{pmatrix}
    \sqrt{3}  & 0\\
    0  & 1\\
    0  & 0
\end{pmatrix}\begin{pmatrix}
    \frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{2}} \\
    -\frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{2}}
\end{pmatrix}</script><h3 id="SVD的一些性质"><a href="#SVD的一些性质" class="headerlink" title="SVD的一些性质"></a>SVD的一些性质</h3><p>对于奇异值，他跟我们特征分解中的特征值类似，在奇艺置矩阵中也是按照从大到小排列，而且奇异值的减少特别快，在很多情况下，前10\%甚至1\%的奇异值就占了全部的奇异值之和的99\%以上的比例。也就是说，我们也可以用最大的k个奇异值和对应的左右奇异向量来近似描述矩阵(与前面描述的题外延伸之矩阵压缩类似)，由于这个重要的性质，SVD也可以用于PCA降维，来做数据压缩和去噪，也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需要来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
<blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/29846048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29846048</a> </p>
<p>参考：<a href="https://www.csuldw.com/2017/03/09/2017-03-09-svd/" target="_blank" rel="noopener">https://www.csuldw.com/2017/03/09/2017-03-09-svd/</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">chennan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://zhuanlan.zhihu.com/chennan" title="https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;chennan" rel="noopener" target="_blank">知乎专栏</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://mp.csdn.net/console/article" title="https:&#x2F;&#x2F;mp.csdn.net&#x2F;console&#x2F;article" rel="noopener" target="_blank">CSDN</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chennan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">74k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:08</span>
</div>

<!--添加运行时间-->
<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* 
		Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数
        */
		var t1 = Date.UTC(2020,06,25,08,00,00); //北京时间2018-2-13 00:00:00
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}
	siteTime();
</script>
<!--添加运行时间END-->


<!-- 新增访客统计代码 -->
<div class="busuanzi-count">
    <script async="" src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="site-uv">
      <i class="fa fa-user"></i>
      访问量 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 次
      <span class="post-meta-divider">|</span>
    </span>
    <div class="powered-by"></div>
    <span class="site-uv">
      <i class="fa fa-eye"></i>
      总访客 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 人
    </span>
    <!--
    <span class="site-pv">
      <i class="fa fa-pencil"></i>
      本站博客共 <span class="post-count">27.2k</span> 字
    </span>
    -->
</div>
<!-- 新增访客统计代码 END-->
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  
  <script type="text/javascript" color="0,0,0" opacity='0.4' zIndex="-2" count="100" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  
</body>
</html>
