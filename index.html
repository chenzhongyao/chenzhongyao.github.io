<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"chenzhongyao.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="chenzhongyao&#39;s website">
<meta property="og:url" content="https://chenzhongyao.github.io/index.html">
<meta property="og:site_name" content="chenzhongyao&#39;s website">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="chennan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://chenzhongyao.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>chenzhongyao's website</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">chenzhongyao's website</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/22/%E9%82%AF%E9%83%B8%E8%B4%AD%E6%88%BF%E7%BC%B4%E7%A8%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/22/%E9%82%AF%E9%83%B8%E8%B4%AD%E6%88%BF%E7%BC%B4%E7%A8%8E/" class="post-title-link" itemprop="url">邯郸购房缴税</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-22 09:08:12 / 修改时间：09:15:11" itemprop="dateCreated datePublished" datetime="2020-06-22T09:08:12+08:00">2020-06-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="邯郸"><a href="#邯郸" class="headerlink" title="邯郸:"></a>邯郸:</h2><h4 id="契税"><a href="#契税" class="headerlink" title="契税"></a>契税</h4><p>契税一般是3%，如果是首次购房，可以开具首次购房证明，如果购买的房子面积小于90平米，可以按1%缴纳，如果面积大于90平米小于140平米，可以按照1.5%缴纳，如果大于140平按3%。如果是2套房,一律按3%缴纳.</p>
<h4 id="印花税"><a href="#印花税" class="headerlink" title="印花税"></a>印花税</h4><p>印花税（0.1%,买卖双方各0.05％），印花税是针对合同或者具有合同性质的凭证，产权转移书据，营业账簿，权利、许可证照和经财政部确定征税的其他凭证所收的一类税费。对于购房者而言，印花税的税率是0.05％，即购房者应纳税额为计税价格×0.05％的数值，印花税采取由纳税人自行缴纳完税的方式。</p>
<h4 id="手续费-查到两个-差不多"><a href="#手续费-查到两个-差不多" class="headerlink" title="手续费(查到两个,差不多)"></a>手续费(查到两个,差不多)</h4><p>1.房屋买卖手续费：120平方米以下1000元，120平方米以上3000元，买卖双方各负担一半。</p>
<p>2.二手房交易手续费总额：住宅6元/平米*实际测绘面积，非住宅10元/平米</p>
<h4 id="营业税"><a href="#营业税" class="headerlink" title="营业税"></a>营业税</h4><p>营业税由城市维护建设税、教育费附加、地方教育附加和销售营业税组成，征收税率为5.6％，个人购买超过２年（含２年）的普通住宅对外销售的，免征营业税。</p>
<h4 id="测绘费"><a href="#测绘费" class="headerlink" title="测绘费"></a>测绘费</h4><p>测绘费：1.36元/平米　　总额=1.36元/平米*实际测绘面积(08年4月后新政策房改房测绘费标准：面积75平米以下收200元，75平米以上144平米以下收300元，144平米以上收400元)一般说来房改房都是需要测绘的，商品房如果原产权证上没有济南市房管局的测绘章也是需要测绘的。</p>
<h4 id="登记费"><a href="#登记费" class="headerlink" title="登记费"></a>登记费</h4><p>登记费：(工本费)80元, 共有权证:20元。　</p>
<p>所需材料：　　⑴地税局需要卖方夫妻双方身份证和户口本复印件一套(若卖方夫妻不在同一个户口本上还需提供结婚证复印件一套)、买方身份证复印件一套、网签买卖协议一份、房产证复印件一套(如果卖方配偶已经去世还需要派出所的死亡证明一份)　　⑵房管局需要网签买卖协议一份、房产证原件、新测绘图纸两张，免税证明或完税证明复印件;如省直房改房还需已购公房确认表原件两份和附表一。　　注：房改房过户时需要配偶一起出面签字;若配偶已经去世但使用了其工龄，如果是在房改之后则需要先做继承公证再交易过户;如在房改之前，则应提交派出所开具的死亡证明原件。省直房改房还需填写《已购公房确认表》两份并由单位和省直房改办盖章确认，并提交房改原始票据原件</p>
<h4 id="公证费"><a href="#公证费" class="headerlink" title="公证费"></a>公证费</h4><p>公证处房产收费是按房屋的市场价格来收费的，具体收费标准如下：<br>一：证明土地使用权出让、转让，房屋转让、买卖及股权转让 标的额500000元以下部分，收取比例为0．3％，按比例收费不到200元的，按200元收取，500001元至5000000元部分，收取0．25％，5000001元至10000000元部分， 收取0.2％，10000001元至20000000元部分，收取0.15％，20000001元至50000000元部分，收取0. 1％，50000001元至100000000元部分，收取0．05％，100000001元以上部分，收取0. 01％。</p>
<p>假如120万购买的房屋,需缴纳500000<em>0.3% + 700000</em>0.25%=3250元</p>
<h4 id="个人所得税-卖方缴纳"><a href="#个人所得税-卖方缴纳" class="headerlink" title="个人所得税(卖方缴纳)"></a>个人所得税(卖方缴纳)</h4><p>个人所得税在房产交易过程中需要由卖方缴纳，缴纳比例固定，但是也存在个人所得税减免情况。</p>
<p>个人所得税：(税率交易总额1%或两次交易差的20%卖方缴纳)征收条件以家庭为单位出售非唯一住房需缴纳个人房转让所得税。在这里有两个条件①家庭唯一住宅②购买时间超过5年。如果两个条件同时满足可以免交个人所得税;任何一个条件不满足都必须缴纳个人所得税。</p>
<ul>
<li>计算方式1:</li>
</ul>
<p>纳税人(卖方)能在地税系统中查到房屋原值，或能提供房屋原值等费用，个人所得税计算方法为：</p>
<p>个人所得税=(计税价格-房屋原值-原契税-本次交易所缴纳税等合理费用)×20%。</p>
<p>　　举例：如果卖方出卖不满是“满五唯一：的房子，计税价格为100万，原值、原契税以及相关税费合计70万元。那么，卖方需要缴纳的个人所得税为：(100-70)×20%=6万元。</p>
<ul>
<li>计算方式2:</li>
</ul>
<p>纳税人(卖方)不能在地税系统中查到房屋原值，也不能提供房屋原值等费用，个人所得税计算方法为：个人所得税=计税价格×1%。</p>
<p>　　举例：卖家不满五或不唯一的住房出售价为100万售。买家承担税费，全额的1%。那么，卖方需要缴纳的个人所得税为：100×1%=1万元。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/20/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/20/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-20 20:11:59" itemprop="dateCreated datePublished" datetime="2020-06-20T20:11:59+08:00">2020-06-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/20/%E5%86%9B%E9%98%9F%E6%96%87%E8%81%8C%E5%8E%86%E5%B9%B4%E8%80%83%E8%AF%95%E7%9C%9F%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/20/%E5%86%9B%E9%98%9F%E6%96%87%E8%81%8C%E5%8E%86%E5%B9%B4%E8%80%83%E8%AF%95%E7%9C%9F%E9%A2%98/" class="post-title-link" itemprop="url">军队文职历年考试真题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-06-20 13:42:28 / 修改时间：20:12:21" itemprop="dateCreated datePublished" datetime="2020-06-20T13:42:28+08:00">2020-06-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="2013年军队文职真题及答案"><a href="#2013年军队文职真题及答案" class="headerlink" title="2013年军队文职真题及答案"></a>2013年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2013年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2014年军队文职真题及答案"><a href="#2014年军队文职真题及答案" class="headerlink" title="2014年军队文职真题及答案"></a>2014年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2014年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2015年军队文职真题及答案"><a href="#2015年军队文职真题及答案" class="headerlink" title="2015年军队文职真题及答案"></a>2015年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2015年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2016年军队文职真题及答案"><a href="#2016年军队文职真题及答案" class="headerlink" title="2016年军队文职真题及答案"></a>2016年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2016年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2018年军队文职真题及答案"><a href="#2018年军队文职真题及答案" class="headerlink" title="2018年军队文职真题及答案"></a>2018年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2018年军队文职真题及答案解析.pdf" data-height="600px"></div>
<h2 id="2019年军队文职真题及答案"><a href="#2019年军队文职真题及答案" class="headerlink" title="2019年军队文职真题及答案"></a>2019年军队文职真题及答案</h2><div class="pdfobject-container" data-target="2019年军队文职真题及答案解析.pdf" data-height="600px"></div>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/RNN%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/RNN%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/" class="post-title-link" itemprop="url">RNN前向传播、反向传播与并行计算（非常详细）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:56" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:56+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RNN前向传播、反向传播与并行计算-非常详细"><a href="#RNN前向传播、反向传播与并行计算-非常详细" class="headerlink" title="RNN前向传播、反向传播与并行计算(非常详细)"></a>RNN前向传播、反向传播与并行计算(非常详细)</h1><h2 id="1-RNN前向传播"><a href="#1-RNN前向传播" class="headerlink" title="1. RNN前向传播"></a>1. RNN前向传播</h2><p>在介绍RNN之前，首先比较一下RNN与CNN的区别：</p>
<ul>
<li>RNN是一类用于处理序列数据的神经网络，CNN是一类用于处理网格化数据(如一幅图像)的神经网络。</li>
<li>RNN可以扩展到更长的序列，大多数RNN也能处理可变长度的序列。CNN可以很容易地扩展到具有很大宽度和高度的图像，并且可以处理可变大小的图像。</li>
</ul>
<p><img src="./images/RNN-前向.jpg" alt="RNN示意图"><br>RNN的前向传播如图所示，其中$f(x)$代表激活函数，输出的label可以使用one-hot形式。图中所有的$U、W、V、b_1、b_2$全部相同，类似于CNN中的权值共享。CNN通过权值共享可以处理任意大小的图片，RNN通过权值共享，可以处理任意序列长度的语音、句子。</p>
<p>损失函数：</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^{t}||o_i-\hat{o}_i||^2=J_1+J_2+...+J_t（J_i为MSE损失或CE损失）</script><h2 id="2-RNN反向传播"><a href="#2-RNN反向传播" class="headerlink" title="2.RNN反向传播"></a>2.RNN反向传播</h2><p>在介绍RNN反向传播之前，先回顾一下基本神经元的反向传播算法：<br><img src="./images/base.png" alt></p>
<script type="math/tex; mode=display">
\begin{array}{l}\left\{ \begin{matrix}
h=&WX+b\\
S=&f(h)
\end{matrix}\right.
\end{array}</script><p>假设已知损失对$S$的梯度$\frac{\partial J}{\partial S}$:</p>
<script type="math/tex; mode=display">
\begin{array}{l}\left\{ \begin{matrix}
\frac{\partial J}{\partial h}=\frac{\partial J}{\partial S}\frac{d S}{d h}\\
\\
\frac{\partial J}{\partial X}=\frac{\partial J}{\partial h}W^T\\ \\
\frac{\partial J}{\partial W}=X^T\frac{\partial J}{\partial h}\\ \\
\frac{\partial J}{\partial b}=SumCol(\frac{\partial J}{\partial h})

\end{matrix}\right.
\end{array}</script><p>具体推导过程请参考：<a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a></p>
<p>下面介绍RNN的反向传播，如图所示：<br><img src="./images/RNN_bp1.png" alt><br><img src="./images/RNN_bp2.png" alt><br><img src="./images/RNN_bp3.jpg" alt><br>因为共享权重，所以整个RNN网络对$V、W、U$的梯度为:</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial V}=\sum_{i=1}^{t} s_{i}^{T} \frac{\partial J}{\partial o_{i}}; \quad \frac{\partial J}{\partial W}=\sum_{i=1}^{t-1} s_{i}^{T} \frac{\partial J}{\partial h_{i+1}}; \quad \frac{\partial J}{\partial U}=\sum_{i=1}^{t} x_{i}^{T} \frac{\partial J}{\partial h_{i}}</script><h2 id="3-RNN并行加速计算"><a href="#3-RNN并行加速计算" class="headerlink" title="3. RNN并行加速计算"></a>3. RNN并行加速计算</h2><h3 id="3-1-前向并行运算"><a href="#3-1-前向并行运算" class="headerlink" title="3.1 前向并行运算"></a>3.1 <strong>前向并行运算</strong></h3><p>因为RNN为延时网络，网络的每个输入都与前一个时刻的输出有关系，因此，当输入只有一句话时，无法并行计算。当有输入为一个batch时，如何并行计算呢？<br><img src="./images/RNN-并行1.png" alt></p>
<p><img src="./images/RNN-并行2.jpg" alt></p>
<p>也就是说，可以将一个batch的样本在某一个时刻的输入输出并行，加速计算，而不是将一个样本的整个过程并行（因为依赖性无法并行）。</p>
<h3 id="3-2-反向并行计算"><a href="#3-2-反向并行计算" class="headerlink" title="3.2 反向并行计算"></a>3.2 <strong>反向并行计算</strong></h3><p>反向并行运算方式如下图所示：<br><img src="./images/RNN-并行3.jpg" alt></p>
<h2 id="4-双向RNN"><a href="#4-双向RNN" class="headerlink" title="4. 双向RNN"></a>4. 双向RNN</h2><p><img src="./images/RNN-双向.jpg" alt><br>注：图中的$W与\hat{W}$、$U与\hat{U}$、$V与\hat{V}$不同。</p>
<h2 id="5-DeepRNN"><a href="#5-DeepRNN" class="headerlink" title="5. DeepRNN"></a>5. DeepRNN</h2><p><img src="./images/RNN-DeepRnn.png" alt></p>
<p>参考资料：深度之眼</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/CNN%E7%9A%84%E5%89%8D%E5%90%91%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/CNN%E7%9A%84%E5%89%8D%E5%90%91%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" class="post-title-link" itemprop="url">四张图彻底搞懂CNN反向传播算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:11:59" itemprop="dateModified" datetime="2020-06-20T20:11:59+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="四张图彻底搞懂CNN反向传播算法"><a href="#四张图彻底搞懂CNN反向传播算法" class="headerlink" title="四张图彻底搞懂CNN反向传播算法"></a>四张图彻底搞懂CNN反向传播算法</h1><p>阅读本文之前，请首先阅读之前讲述的全连接层的反向传播算法详细推导过程： <a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a><br>已经了解反向传播算法请自动忽略。</p>
<h2 id="1-卷积层的反向传播"><a href="#1-卷积层的反向传播" class="headerlink" title="1. 卷积层的反向传播"></a>1. 卷积层的反向传播</h2><p>直接上图：<br><img src="./images/cnn.png" alt><br>假设输入为一张单通道图像$x$，卷积核大小为$2\times 2$，输出为$y$。为了加速计算，首先将$x$按卷积核滑动顺序依次展开，如上图所示。其中，$\hat{x}$中的红色框代表$x$中的红色框展开后的结果，将$x$依次按照此方式展开，可得$\hat{x}$。同理可得$\hat{w}$，然后通过矩阵相乘可得输出$\hat{y}$（$\hat{y}$与$y$等价）。此时，已经将CNN转化为FC，反向传播算法与<a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a> 完全一致，这里不再做详细介绍。当输入图像有N个样本时，应该怎么算呢？</p>
<p>当有$N$个样本，即batch=N时，前向与反向传播方式如下图所示：<br><img src="./images/cnn-batch.png" alt><br>其中，输入图像batch=3,使用2个$2\times 2\times 3$的卷积核，输出两张图像，如图所示。红色框、黄色框代表的是卷积核以及使用该卷积核得到的输出图像$y$。当输入图像为一个batch时，$x、w$的转化方式如上图，首先将输入图像与卷积核分别按单通道图像展开，然后将展开后的矩阵在行方向级联。此时，已经将CNN转化为了FC，反向传播算法与<a href="https://zhuanlan.zhihu.com/p/79657669" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669</a> 完全一致，这里不再做过多介绍。</p>
<h2 id="2-Average-pooling的反向传播"><a href="#2-Average-pooling的反向传播" class="headerlink" title="2. Average pooling的反向传播"></a>2. Average pooling的反向传播</h2><p><img src="./images/average-pooling.png" alt><br>$\frac{\partial J}{\partial w}$不用求，因为$w$为常数。$\frac{\partial J}{\partial x<em>{ij}}=\Sigma \frac{\partial J}{\partial \hat{x}</em>{ij}}$</p>
<h2 id="3-Max-pooling的反向传播"><a href="#3-Max-pooling的反向传播" class="headerlink" title="3. Max-pooling的反向传播"></a>3. Max-pooling的反向传播</h2><p><img src="./images/max-pooling.png" alt><br>遍历$\hat{x}$的每一行，找出此行最大值的索引$(i,j)$，然后将$\frac{\partial J}{\partial \hat{x}}$中索引为$(i,j)$的值设为$\frac{\partial J}{\partial \hat{y}}$对应行的值，将此行其余列的值设为0，如上图所示红框所示。假设$\hat{x}$中(1,1)处的值是第一行中最大的值，则将$\frac{\partial J}{\partial y<em>{11}}$赋值给$\frac{\partial J}{\partial \hat{x}}$中索引为$(1,1)$的位置。最后计算:$\frac{\partial J}{\partial x</em>{ij}}=\Sigma \frac{\partial J}{\partial \hat{x}_{ij}}$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/LDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/LDA/" class="post-title-link" itemprop="url">线性判别分析LDA原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="线性判别分析LDA原理"><a href="#线性判别分析LDA原理" class="headerlink" title="线性判别分析LDA原理"></a>线性判别分析LDA原理</h2><p>线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了解下它的算法原理。</p>
<h3 id="1-LDA的思想"><a href="#1-LDA的思想" class="headerlink" title="1. LDA的思想"></a>1. LDA的思想</h3><p>LDA的思想是：<font color="#FF0000"> <strong>最大化类间均值，最小化类内方差</strong></font>。意思就是将数据投影在低维度上，并且投影后同种类别数据的投影点尽可能的接近，不同类别数据的投影点的中心点尽可能的远。</p>
<p>我们先看看最简单的情况。假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。<br><img src="./images/LDA.jpg" alt></p>
<p>上图提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。</p>
<h3 id="2-瑞利商-Rayleigh-quotient-与广义瑞利商-genralized-Rayleigh-quotient"><a href="#2-瑞利商-Rayleigh-quotient-与广义瑞利商-genralized-Rayleigh-quotient" class="headerlink" title="2. 瑞利商(Rayleigh quotient)与广义瑞利商(genralized Rayleigh quotient)"></a>2. 瑞利商(Rayleigh quotient)与广义瑞利商(genralized Rayleigh quotient)</h3><p>首先来看瑞利商的定义。瑞利商是指这样的函数$R(A,x)$:</p>
<script type="math/tex; mode=display">
R(A, x)=\frac{x^{H} A x}{x^{H} x}</script><p>其中，$x$是非零向量，而$A$为$n\times n$的Hermitan矩阵。所谓的Hermitan矩阵就是满足<strong>共轭转置矩阵</strong>和自己相等的矩阵，即$A^H=A$，例如，$A=\left(\begin{array}{cc}{1} &amp; {2+i} \ {2-i} &amp; {1}\end{array}\right)$的共轭转置等于其本身。当$A$为实矩阵时，如果满足$A^T=A$，则$A$为Hermitan矩阵。</p>
<p>瑞利商$R(A,x)$有一个非常重要的性质，即<font color="#FF0000"><strong>它的最大值等于矩阵$A$最大的特征值，而最小值等于矩阵$A$的最小的特征值</strong></font>，也就是满足</p>
<font color="#FF0000">
$$
\lambda_{\min } \leq \frac{x^{H} A x}{x^{H} x} \leq \lambda_{\max }
$$
</font>

<p>当向量$x$是标准正交基，即满足$x^{H} x=1$时，瑞利商退化为$R(A, x)=x^{H} A x$。</p>
<p>下面看一下广义瑞利商。广义瑞利商是指这样的函数$R(A,B,x)$:</p>
<script type="math/tex; mode=display">
R(A,B, x)=\frac{x^{H} A x}{x^{H} B x}</script><p>其中x为非零向量，而$A,B$为$n\times n$的Hermitan矩阵。$B$为正定矩阵。它的最大值和最小值是什么呢？其实我们只要通过将其通过标准化就可以转化为瑞利商的格式。令$x=B^{-1 / 2} x^{\prime}$，则分母转化为：</p>
<script type="math/tex; mode=display">
x^{H} B x=x^{\prime H}\left(B^{-1 / 2}\right)^{H} B B^{-1 / 2} x^{\prime}=x^{\prime H} B^{-1 / 2} B B^{-1 / 2} x^{\prime}=x^{\prime H} x^{\prime}</script><p>而分子转化为：</p>
<script type="math/tex; mode=display">
x^{H} A x=x^{\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\prime}</script><p>此时我们的$R(A,B,x)$转化为$R(A,B,x’)$:</p>
<script type="math/tex; mode=display">
R\left(A, B, x^{\prime}\right)=\frac{x^{\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\prime}}{x^{\prime H} x^{\prime}}</script><p>利用前面的瑞利商的性质，我们可以很快的知道，$R(A,B,x’)$的最大值为矩阵$B^{-1 / 2} A B^{-1 / 2}$的最大特征值，或者说矩阵$B^{−1}A$的最大特征值，而最小值为矩阵$B^{−1}A$的最小特征值。</p>
<h3 id="2-LDA的原理及推导过程"><a href="#2-LDA的原理及推导过程" class="headerlink" title="2. LDA的原理及推导过程"></a>2. LDA的原理及推导过程</h3><p>假设样本共有K类，每一个类的样本的个数分别为$N<em>1,N_2,…,N_k$ \<br>$x_1^1,x_1^2,…,x_1^{N_1}$对应第1类\<br>$x_2^1,x_2^2,…,x_2^{N_2}$对应第2类\<br>$x_k^1,x_k^2,…,x_k^{N_k}$对应第K类，其中每个样本$x_i^j$均为$n$维向量 \<br>设$\tilde{x}</em>{i}^{j}$为$x<em>i^j$变化后的样本，则$\tilde{x}</em>{i}^{j}=<x, u>u=|x||u|cos\theta \cdot u =\left(x^{T} u\right) u$</x,></p>
<p>此处设$u$为单位向量，即$u^Tu=1$.</p>
<p>假设第K类样本的数据集为$D<em>k$，变化后的样本的均值向量为：$\tilde{\mathrm{m}}=\frac{\sum</em>{\tilde{x}\in D_k}\tilde{x}}{N_k}$，那么，第K类样本的方差为$\frac{S_k}{N_k}$，其中：</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{S}_{\mathrm{k}} &=\sum_{\tilde{x} \in D_{k}}(\tilde{x}-\tilde{\mathrm{m}})^{T}(\tilde{x}-\tilde{\mathrm{m}})\\ 
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right) u-\left(m^{T} u\right) u\right]^T\left[\left(x^{T} u\right) u -\left(m^{T} u\right) u\right] \\
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right) u^{T}-\left(m^{T} u\right) u^{T}\right]\left[\left(x^{T} u\right) u -\left(m^{T} u\right) u\right]  \quad (x^Tu，m^Tu为实数,转置仍是本身)\\ 
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right)^{2} u^{T} u-2\left(x^{T} u\right)\left(m^{T} u\right) u^{T} u+\left(m^{T} u\right)^{2} u^{T} u\right] \\ 
&=\sum_{x \in D_{k}}\left[\left(x^{T} u\right)^{2}-2\left(x^{T} u\right)\left(m^{T} u\right)+\left(m^{T} u\right)^{2}\right] \end{aligned}</script><p>第K类样本的方差：</p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{S_{k}}{N_{k}} &=\frac{\sum_{x \in D_{k}}\left(x^{T} u\right)^{2}}{N_{k}}-2 \frac{\sum_{x \in D_{k}} x^{T}\left(u m^{T} u\right)}{N_{k}}+\frac{\sum_{x\in D_{k}}\left(m^{T} u\right)^{2}}{N_{k}} \\ 
&=\frac{\sum_{x\in D_k} u^{T} x x^{T} u}{N_{k}}-2 \frac{\sum_{x\in D_k} x^{T}}{N_{k}} u m^{T} u+\left(m^{T} u\right)^{2}\quad (注：\frac{\sum_{x\in D_{k}}\left(m^{T} u\right)^{2}}{N_{k}}=(m^Tu)^2。因为m^Tu与x无关)\\ 
&=u^{T} \frac{\sum_{x\in D_k} x x^{T}}{N_{k}} u-\left(m^{T} u\right)^{2} \quad (注：\frac{\sum_{x \in D_k}x^T}{N_k}=m^T)\\ 
&=u^{T} \frac{\sum_{x\in D_k} x x^{T}}{N_{k}} u-u^{T} m m^{T} u \\ 
&=u^{T}\left(\frac{\sum_{x\in D_k} x x^{T}}{N_{k}}-m m^{T}\right) u \end{aligned}</script><p>各个类别的样本方差之和：</p>
<script type="math/tex; mode=display">
\begin{aligned} \sum_{k}\frac{S_k}{N_k} &=\sum_{k=1}^{K}u^{T}\left(\frac{\sum_{x \in D_k} x x^{T}}{N_{k}}-m_k m_k^{T}\right) u \\ &=u^T\sum_{k=1}^{K}(\frac{\sum_{x \in D_k}xx^T}{N_k}-m_km_k^T)u\\&=u^TS_wu
\end{aligned}</script><p>其中，$S<em>w=\sum</em>{k=1}^{K}(\frac{\sum_{x \in D_k}xx^T}{N_k}-m_km_k^T)$，$S_w$一般被称为类内散度矩阵</p>
<p>不同类别$i,j$之间的中心距离：</p>
<script type="math/tex; mode=display">
\begin{aligned} S_{i j} &=\left(\tilde{m}_{i}-\tilde{m}_{j}\right)^{T}\left(\tilde{m}_{i}-\tilde{m}_{j}\right) \\
&=[(u^Tm_i)u-(u^Tm_j)u]^T[(m_i^Tu)u-(m_j^Tu)u] \\
&=[(u^Tm_i)u^T-(u^Tm_j)u^T][u(m_i^Tu)-u(m_j^Tu)] \\
&=u^T(m_i-m_j)u^Tu(m_i^T-m_j^T)u \\
&=u^{T}\left(m_{i}-m_{j}\right)\left(m_{i}-m_{j}\right)^{T} u \end{aligned}</script><p>所有类别之间的距离之和为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \sum_{i, j \atop i \neq j} s_{i j} &= u^T\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]u \\
    &=u^TS_bu
\end{aligned}</script><p>其中，$S<em>b=\sum</em>{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]$，$S_b$一般被称为类间散度矩阵。</p>
<p>在已知条件下，$S_w,S_b$均可求出，LDA算法的目标是“<strong>类间距离尽可能大，类内方差尽可能小</strong>”，即最大化$u^TS_bu$,最小化$u^TS_wu$.</p>
<p>令$J(u)=\frac{u^{T} S<em>{b} u}{u^{T} S</em>{w} u}$,则目标函数为：</p>
<script type="math/tex; mode=display">
\max J(u)</script><p>为了使所求最大，可假设$u^TS_wu=1$，则问题转化为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\max u^{T} S_{b} u} \\
s.t. \quad u^TS_wu=1 
\end{array}</script><script type="math/tex; mode=display">
\begin{aligned}
 L(u, \lambda)&=u^{T} S_{b} u+\lambda\left(1-u^{T} S_{w} u\right) \\
 \frac{\partial L}{\partial u} &= S_bu+S_b^Tu-\lambda S_wu-\lambda S_w^Tu \\ &=2(S_bu-\lambda S_wu)\quad (因为S_b，S_w为对称矩阵)\\ &=0 \Rightarrow S_{b} u=\lambda S_{w} u \end{aligned}</script><blockquote>
<p>证明$S_b$为对称矩阵：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    S_b^T &= (\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T])^T\\
    &=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]^T \\
    &=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T] \\
    &=S_b
\end{aligned}</script><p>所以，$S_b$为对称矩阵，同理可证明$S_w$为对称矩阵。</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{array}{l}
S_{w}^{-1}{S_{b}  u=\lambda u}
\end{array}</script><p>计算矩阵$S_w^{-1}S_b$的最大的$d$个特征值和对应的$d$个特征向量$(w_1,w_2,…,w_d)$，得到投影矩阵$W=(w_1,w_2,…,w_d)$。</p>
<blockquote>
<p>注意: (1)选取特征值时，如果一些特征值明显大于其他的特征值，则取这些取值较大的特征值，因为它们包含更多的数据分布的信息。相反，如果一些特征值接近于0，我们将这些特征值舍去。<br>(2)由于$W$是一个利用了样本类别得到的投影矩阵，因此它能够降维到的维度d的最大值为$K-1$。为什么不是$K$呢？因为$S_b$中每个$m_i-m_j$的秩均为1(因为$m_i$为1维向量).</p>
<p>由$R(AB)\leq \min(R(A), R(B))$知，</p>
<script type="math/tex; mode=display">R((m_i-m_j)(m_i-m_j)^T)=1</script><p>又 $\because$</p>
<script type="math/tex; mode=display">S_b=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T], \\R(A+B)\leq R(A)+R(B)</script><script type="math/tex; mode=display">\therefore R(S_b)\leq K-1 \Rightarrow R(S_w^{-1}S_b)\leq K-1</script><p>$\therefore \lambda =0$对应的特征向量至少有$(K-(K-1))=1$个，也就是说$d$最大为$K-1$. </p>
</blockquote>
<h3 id="3-LDA算法流程"><a href="#3-LDA算法流程" class="headerlink" title="3. LDA算法流程"></a>3. LDA算法流程</h3><p>输入：数据集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$,其中，任意样本$x_i$为$n$维向量，$y_i\in {C_1,C_2,…,C_k}$,降维到的维度为$d$.</p>
<p>输出：降维后的数据集$D’$.</p>
<ol>
<li>计算类内散度矩阵$S_w$</li>
<li>计算类间散度矩阵$S_b$</li>
<li>计算矩阵$S_w^{-1}S_b$</li>
<li>计算矩阵$S_w^{-1}S_b$的特征值与特征向量，按从小到大的顺序选取前$d$个特征值和对应的$d$个特征向量$(w_1,w_2,…,w_d)$，得到投影矩阵$W$.</li>
<li>对样本集中的每一个样本特征$x_i$，转化为新的样本$z_i=W^Tx_i$</li>
<li>得到输出样本集$D’={(z_1,y_1),(z_2,y_2),…,(z_m,y_m)}$<h3 id="4-LDA与PCA对比"><a href="#4-LDA与PCA对比" class="headerlink" title="4. LDA与PCA对比"></a>4. LDA与PCA对比</h3>LDA与PCA都可用于降维，因此有很多相同的地方，也有很多不同的地方</li>
</ol>
<ul>
<li><p><strong>相同点：</strong></p>
<ul>
<li>两者均可用于数据降维</li>
<li>两者在降维时均使用了矩阵特征分解的思想</li>
<li>两者都假设数据符合高斯分布</li>
</ul>
</li>
<li><p><strong>不同点：</strong></p>
<ul>
<li>LDA是有监督的降维方法，而PCA是无监督降维方法</li>
<li>当总共有K个类别时，LDA最多降到K-1维，而PCA没有这个限制</li>
<li>LDA除了用于降维，还可以用于分类</li>
<li>LCA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优（如下图的左图）。当然，某些数据分布下PCA比LDA降维较优（如下图的右图）。<table align="center">
<tr>
<td><center><img height="200" src="./images/LDA-2.jpg"></center>图2</td>
<td><center><img height="200" src="./images/LDA-3.jpg"></center>图3</td>
</tr>
</table>

</li>
</ul>
</li>
</ul>
<h3 id="5-LDA算法小结"><a href="#5-LDA算法小结" class="headerlink" title="5. LDA算法小结"></a>5. LDA算法小结</h3><p>LDA算法既可以用来降维，也可以用来分来，但是目前来说，LDA主要用于降维，在进行与图像识别相关的数据分析时，LDA是一个有力的工具。下面总结一下LDA算法的优缺点。</p>
<ul>
<li><strong>LDA算法的优点</strong><ul>
<li>在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习无法使用先验知识；</li>
<li>LDA在样本分类信息依赖均值而不是方差的时候，比PCA算法较优。</li>
</ul>
</li>
<li><strong>LDA算法的缺点</strong><ul>
<li>LDA与PCA均不适合对非高斯分布样本进行降维</li>
<li>LDA降维算法最多降到类别数K-1的维度，当降维的维度大于K-1时，则不能使用LDA。当然目前有一些改进的LDA算法可以绕过这个问题</li>
<li>LDA在样本分类信息依赖方差而非均值的时候，降维效果不好</li>
<li>LDA可能过度拟合数据<blockquote>
<p>参考：<a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6244265.html</a></p>
</blockquote>
</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/LSTM/" class="post-title-link" itemprop="url">LSTM前向传播与反向传播算法推导（非常详细）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="LSTM前向传播与反向传播算法推导（非常详细）"><a href="#LSTM前向传播与反向传播算法推导（非常详细）" class="headerlink" title="LSTM前向传播与反向传播算法推导（非常详细）"></a>LSTM前向传播与反向传播算法推导（非常详细）</h2><h3 id="1-长短期记忆网络LSTM"><a href="#1-长短期记忆网络LSTM" class="headerlink" title="1.长短期记忆网络LSTM"></a>1.长短期记忆网络LSTM</h3><p>LSTM(Long short-term memory)通过刻意的设计来避免长期依赖问题，是一种特殊的RNN。长时间记住信息实际上是 LSTM 的默认行为，而不是需要努力学习的东西！</p>
<p>所有递归神经网络都具有神经网络的链式重复模块。在标准的RNN中，这个重复模块具有非常简单的结构，例如只有单个tanh层，如下图所示。<br><img src="./images/lstm-rnn.jpg" alt><br>LSTM具有同样的结构，但是重复的模块拥有不同的结构，如下图所示。与RNN的单一神经网络层不同，这里有四个网络层，并且以一种非常特殊的方式进行交互。<br><img src="./images/lstm.jpg" alt></p>
<h4 id="1-1-LSTM—遗忘门"><a href="#1-1-LSTM—遗忘门" class="headerlink" title="1.1 LSTM—遗忘门"></a>1.1 LSTM—遗忘门</h4><p><img src="./images/lstm-1.jpg" alt><br>LSTM 的第一步要决定从细胞状态中舍弃哪些信息。这一决定由所谓“遗忘门层”的 S 形网络层做出。它接收 $h<em>{t-1}$ 和 $x_t$，并且对细胞状态 $C</em>{t−1}$ 中的每一个数来说输出值都介于 0 和 1 之间。1 表示“完全接受这个”，0 表示“完全忽略这个”。</p>
<h4 id="1-2-LSTM—输入门"><a href="#1-2-LSTM—输入门" class="headerlink" title="1.2 LSTM—输入门"></a>1.2 LSTM—输入门</h4><p><img src="./images/lstm-2.jpg" alt><br>下一步就是要确定需要在细胞状态中保存哪些新信息。这里分成两部分。第一部分，一个所谓“输入门层”的 S 形网络层确定哪些信息需要更新。第二部分，一个 tanh 形网络层创建一个新的备选值向量—— $\tilde{C}_t$，可以用来添加到细胞状态。在下一步中我们将上面的两部分结合起来，产生对状态的更新。</p>
<h4 id="1-3-LSTM—细胞状态更新"><a href="#1-3-LSTM—细胞状态更新" class="headerlink" title="1.3 LSTM—细胞状态更新"></a>1.3 LSTM—细胞状态更新</h4><p><img src="./images/lstm-3.jpg" alt><br>现在更新旧的细胞状态 $C_{t−1}$ 更新到 $C_t$。先前的步骤已经决定要做什么，我们只需要照做就好。<br>我们对旧的状态乘以 $f_t$，用来忘记我们决定忘记的事。然后我们加上 $i_t\odot\tilde{C}_t$，这是新的候选值，根据我们对每个状态决定的更新值按比例进行缩放。</p>
<h4 id="1-4-LSTM—输出门"><a href="#1-4-LSTM—输出门" class="headerlink" title="1.4 LSTM—输出门"></a>1.4 LSTM—输出门</h4><p><img src="./images/lstm-4.jpg" alt><br>最后，我们需要确定输出值。输出依赖于我们的细胞状态，但会是一个“过滤的”版本。首先我们运行 S 形网络层，用来确定细胞状态中的哪些部分可以输出。然后，我们把细胞状态输入 tanh（把数值调整到 −1 和 1 之间）再和 S 形网络层的输出值相乘，部这样我们就可以输出想要输出的分。</p>
<h3 id="2-LSTM的变种以及前向、反向传播"><a href="#2-LSTM的变种以及前向、反向传播" class="headerlink" title="2. LSTM的变种以及前向、反向传播"></a>2. LSTM的变种以及前向、反向传播</h3><p>目前所描述的还只是一个相当一般化的 LSTM 网络。但并非所有 LSTM 网络都和之前描述的一样。事实上，几乎所有文章都会改进 LSTM 网络得到一个特定版本。差别是次要的，但有必要认识一下这些变种。</p>
<h4 id="2-1-带有”窥视孔连接”的LSTM"><a href="#2-1-带有”窥视孔连接”的LSTM" class="headerlink" title="2.1 带有”窥视孔连接”的LSTM"></a>2.1 带有”窥视孔连接”的LSTM</h4><p>一个流行的 LSTM 变种由 Gers 和 Schmidhuber 提出，在 LSTM 的基础上添加了一个“窥视孔连接”，这意味着我们可以让门网络层输入细胞状态。<br><img src="./images/lstm-5.jpg" alt><br>上图中我们为所有门添加窥视孔，但许多论文只为部分门添加。</p>
<p><strong>前向传播：</strong><br>在t时刻的前向传播公式为：</p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{l}
    {i_t=\sigma(\tilde{i}_t)=\sigma(W_{xi}x_t+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_i)} \\
    {f_t=\sigma(\tilde{f}_t)=\sigma(W_{xf}x_t+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_f) }\\
    {g_t=\tanh(\tilde{g}_t)=\tanh(W_{xg}x_t+W_{hg}h_{t-1}+b_g)} \\
    {o_t=\sigma(\tilde{o}_t)=\sigma(W_{xo}x_t+W_{ho}h_{t-1}+W_{co}c_{t}+b_o) }\\
    {c_t=c_{t-1}\odot f_t+g_t\odot i_t}\\
    {m_t=\tanh(c_t)}\\
    {h_t=o_t\odot m_t}\\
    {y_t=W_{yh}h_t+b_y}

\end{array}\right.</script><p><strong>反向传播：</strong><br>为了更直观的推导反向传播算法，将上图转化为下图：<br><img src="./images/lstm-bp.png" alt><br>对反向传播算法了解不够透彻的，请参考<a href="https://zhuanlan.zhihu.com/p/79657669，这里有详细的推导过程，本文将直接使用https://zhuanlan.zhihu.com/p/79657669的结论。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79657669，这里有详细的推导过程，本文将直接使用https://zhuanlan.zhihu.com/p/79657669的结论。</a></p>
<p>已知：$\frac{\partial J}{\partial y<em>t},\frac{\partial J}{\partial c</em>{t+1}},\frac{\partial J}{\partial \tilde{o}<em>{t+1}},\frac{\partial J}{\partial \tilde{f}</em>{t+1}},\frac{\partial J}{\partial \tilde{i}<em>{t+1}},\frac{\partial J}{\partial \tilde{g}</em>{t+1}}$,求某个节点梯度时，首先应该找到该节点的输出节点，然后分别计算所有输出节点的梯度乘以输出节点对该节点的梯度，最后相加即可得到该节点的梯度。如计算$\frac{\partial J}{\partial h<em>t}$时，找到$h_t$节点的所有输出节点$y_t、 \tilde{o}</em>{t+1}、\tilde{f}<em>{t+1}、\tilde{i}</em>{t+1}、\tilde{g}<em>{t+1}$，然后分别计算输出节点的梯度(如$\frac{\partial J}{\partial y_t}$)与输出节点对$h_t$的梯度的乘积（如$\frac{\partial J}{\partial y_t}W</em>{yh}^T$），最后相加即可得到节点$h_t$的梯度:</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial h_t}=\frac{\partial J}{\partial y_t}W_{yh}^T+\frac{\partial J}{\partial \tilde{o}_{t+1}}W_{ho}^T+\frac{\partial J}{\partial \tilde{f}_{t+1}}W_{hf}^T+\frac{\partial J}{\partial \tilde{i}_{t+1}}W_{hi}^T+\frac{\partial J}{\partial \tilde{g}_{t+1}}W_{hg}^T</script><p>同理可得t时刻其它节点的梯度：</p>
<script type="math/tex; mode=display">
\left \{\begin{array}{l}
    \frac{\partial J}{\partial h_t}=\frac{\partial J}{\partial y_t}W_{yh}^T+\frac{\partial J}{\partial \tilde{o}_{t+1}}W_{ho}^T+\frac{\partial J}{\partial \tilde{f}_{t+1}}W_{hf}^T+\frac{\partial J}{\partial \tilde{i}_{t+1}}W_{hi}^T+\frac{\partial J}{\partial \tilde{g}_{t+1}}W_{hg}^T \\ \\
    \frac{\partial J}{\partial m_t} = \frac{\partial J}{\partial h_t} \odot o_t \\ \\
    \frac{\partial J}{\partial c_t} = \frac{\partial J}{\partial m_t}\frac{dm_t}{dc_t}+ \frac{\partial J}{\partial c_{t+1}}\odot f_{t+1} +\frac{\partial J}{\partial \tilde{f}_{t+1}}W_{cf}^T+\frac{\partial J}{\partial \tilde{i}_{t+1}}W_{ci}^T \\ \\
    \left. \begin{array}{l}
         \frac{\partial J}{\partial g_t} = \frac{\partial J}{\partial c_t}\odot i_t \\
         \frac{\partial J}{\partial i_t} = \frac{\partial J}{\partial c_t} \odot g_t \\
         \frac{\partial J}{\partial f_t} = \frac{\partial J}{\partial c_t} \odot c_{t-1} \\
         \frac{\partial J}{\partial o_t} = \frac{\partial J}{\partial h_t} \odot m_t
    \end{array} \right \} \Rightarrow \left\{ \begin{array}{l}
        \frac{\partial J}{\partial \tilde{g}_t} = \frac{\partial J}{\partial g_t}(1-g_t^2) \\
        \frac{\partial J}{\partial \tilde{i}_t} = \frac{\partial J}{\partial i_t}i_t(1-i_t) \\
        \frac{\partial J}{\partial \tilde{f}_t} = \frac{\partial J}{\partial f_t}f_t(1-f_t) \\
        \frac{\partial J}{\partial \tilde{o}_t} = \frac{\partial J}{\partial o_t}i_t(1-o_t) \\
    \end{array}\right. \\ \\
    \frac{\partial J}{\partial x_t} = \frac{\partial J}{\partial \tilde{o}_t}W_{xo}^T+\frac{\partial J}{\partial \tilde{f}_t}W_{xf}^T+ \frac{\partial J}{\partial \tilde{i}_t}W_{xi}^T+\frac{\partial J}{\partial \tilde{g}_t}W_{xg}^T\\
\end{array}\right.</script><p>对参数的梯度：</p>
<script type="math/tex; mode=display">
\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{ho}} = h_t^T\frac{\partial J}{\partial \tilde{o}_{t+1}} \\
    \frac{\partial J}{\partial W_{hf}} = h_t^T\frac{\partial J}{\partial \tilde{f}_{t+1}} \\
    \frac{\partial J}{\partial W_{hi}} = h_t^T\frac{\partial J}{\partial \tilde{i}_{t+1}} \\
    \frac{\partial J}{\partial W_{hg}} = h_t^T\frac{\partial J}{\partial \tilde{g}_{t+1}}
\end{array} \right. 

\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{yh}} = h_t^T\frac{\partial J}{\partial y_t} \\
    \frac{\partial J}{\partial W_{cf}} = c_t^T\frac{\partial J}{\partial \tilde{f}_{t+1}} \\
    \frac{\partial J}{\partial W_{ci}} = c_t^T\frac{\partial J}{\partial \tilde{i}_{t+1}} \\
    \frac{\partial J}{\partial W_{co}} = c_t^T\frac{\partial J}{\partial \tilde{o}_{t}} 
\end{array} \right. 

\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{xo}} = x_t^T\frac{\partial J}{\partial \tilde{o}_{t}} \\
    \frac{\partial J}{\partial W_{xf}} = x_t^T\frac{\partial J}{\partial \tilde{f}_{t}} \\
    \frac{\partial J}{\partial W_{xi}} = x_t^T\frac{\partial J}{\partial \tilde{i}_{t}} \\
    \frac{\partial J}{\partial W_{xg}} = x_t^T\frac{\partial J}{\partial \tilde{g}_{t}} \\
\end{array} \right.</script><h4 id="2-2-GRU"><a href="#2-2-GRU" class="headerlink" title="2.2 GRU"></a>2.2 GRU</h4><p>一个更有意思的 LSTM 变种称为 Gated Recurrent Unit（GRU），由 Cho 等人提出。LSTM通过三个门函数输入门、遗忘门和输出门分别控制输入值、记忆值和输出值。而GRU中只有两个门：更新门$z_t$和重置门$r_t$，如下图所示。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多；重置门控制前一时刻状态有多少信息被写入到当前的候选集$\tilde{h}_t$上，重置门越小，前一状态的信息被写入的越少。这样做使得 GRU 比标准的 LSTM 模型更简单，因此正在变得流行起来。<br><img src="./images/lstm-7.jpg" alt><br>为了更加直观的推导反向传播公式，将上图转化为如下形式：<br><img src="./images/lstm-gru.png" alt><br><strong>GRU的前向传播：</strong> 在t时刻的前向传播公式为：</p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{l}
    {r_t=\sigma(\tilde{r}_t)=\sigma(W_{xr}x_t+W_{hr}h_{t-1}+b_r)} \\
    {z_t=\sigma(\tilde{z}_t)=\sigma(W_{xz}x_t+W_{hz}h_{t-1}+b_z) }\\
    {s_t=\tanh(\tilde{s}_t)}=\tanh[W_{xs}x_t+(h_{t-1}\odot r_t)W_{hs}+b_s]  \\
    {h_t=z_t\odot s_t + h_{t-1}\odot (1-z_t)}\\
    {y_t=W_{yh}h_t+b_y}
\end{array}\right.</script><p><strong>GRU的反向传播：</strong><br>t时刻其它节点的梯度:</p>
<script type="math/tex; mode=display">
\left\{ \begin{array}{l}
    \frac{\partial J}{\partial h_t}=\frac{\partial J}{\partial y_t}W_{hy}^T+\frac{\partial J}{\partial \tilde{r}_{t+1}}W_{hr}^T+ \frac{\partial J}{\partial \tilde{z}_{t+1}}W_{hz}^T+ \frac{\partial J}{\partial \tilde{s}_{t+1}}W_{hs}^T \odot  r_t + \frac{\partial J}{\partial \tilde{h}_{t+1}}\odot(1-z_t)\\
\left.\begin{array}{l}
    \frac{\partial J}{\partial s_t}=\frac{\partial J}{\partial h_t}\odot z_t\\
    \frac{\partial J}{\partial z_t}=\frac{\partial J}{\partial h_t}\odot s_t + \frac{\partial J}{\partial h_t}\odot (-h_{t-1})  \\
    \frac{\partial J}{\partial r_t}=\frac{\partial J}{\partial \tilde{s}_t}W_{hs}^T\odot h_{t-1} \\
\end{array}\right\} \Rightarrow
\left\{ \begin{array}{l}
    \frac{\partial J}{\partial \tilde{s}_t}=\frac{\partial J}{\partial s_t}(1-s_t^2)\\
    \frac{\partial J}{\partial \tilde{z}_t}=\frac{\partial J}{\partial z_t}z_t(1-z_t)  \\
    \frac{\partial J}{\partial \tilde{r}_t}=\frac{\partial J}{\partial r_t}r_t(1-r_t) \\
\end{array}\right. 
\\
    \frac{\partial J}{\partial x_t} = \frac{\partial J}{\partial \tilde{r}_t}W_{xr}^T+\frac{\partial J}{\partial \tilde{z}_t}W_{xz}^T+ \frac{\partial J}{\partial \tilde{s}_t}W_{xs}^T\\
\end{array}\right.</script><p>对参数的梯度：</p>
<script type="math/tex; mode=display">
\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{hy}} = h_t^T\frac{\partial J}{\partial y_t} \\
    \frac{\partial J}{\partial W_{hs}} = (h_{t-1}\odot r_t)^T\frac{\partial J}{\partial \tilde{s}_{t}} \\
    \frac{\partial J}{\partial W_{hz}} = h_{t-1}^T\frac{\partial J}{\partial \tilde{z}_{t}} \\
    \frac{\partial J}{\partial W_{hr}} = h_{t-1}^T\frac{\partial J}{\partial \tilde{r}_{t}}
\end{array} \right. 
\left \{\begin{array}{l}
    \frac{\partial J}{\partial W_{xs}} = x_t^T\frac{\partial J}{\partial \tilde{s}_{t}} \\
    \frac{\partial J}{\partial W_{xz}} = x_t^T\frac{\partial J}{\partial \tilde{f}_{z}} \\
    \frac{\partial J}{\partial W_{xr}} = x_t^T\frac{\partial J}{\partial \tilde{r}_{t}} 
\end{array} \right.</script><h4 id="2-3-结合遗忘门与输入门的LSTM"><a href="#2-3-结合遗忘门与输入门的LSTM" class="headerlink" title="2.3 结合遗忘门与输入门的LSTM"></a>2.3 结合遗忘门与输入门的LSTM</h4><p>另一个变种把遗忘和输入门结合起来。同时确定要遗忘的信息和要添加的新信息，而不再是分开确定。当输入的时候才会遗忘，当遗忘旧信息的时候才会输入新数据。<br><img src="./images/lstm-6.jpg" alt><br>前向与反向算法与上述变种相同，这里不再做过多推导。</p>
<blockquote>
<p>参考资料：<a href="https://www.cnblogs.com/xuruilong100/p/8506949.html" target="_blank" rel="noopener">https://www.cnblogs.com/xuruilong100/p/8506949.html</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/" class="post-title-link" itemprop="url">SVD奇异值分解逐步推导</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:09" itemprop="dateModified" datetime="2020-06-20T20:12:09+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="SVD奇异值分解逐步推导"><a href="#SVD奇异值分解逐步推导" class="headerlink" title="SVD奇异值分解逐步推导"></a>SVD奇异值分解逐步推导</h2><h3 id="1-回顾特征值和特征向量"><a href="#1-回顾特征值和特征向量" class="headerlink" title="1. 回顾特征值和特征向量"></a>1. 回顾特征值和特征向量</h3><p>首先回顾下特征值和特征向量的定义：</p>
<script type="math/tex; mode=display">
Ax=\lambda x</script><p>其中，A是一个$n\times n$的矩阵，$x$是一个$n$维向量，则$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$对应的特征向量。</p>
<p>求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$\lambda _1\leq \lambda_2\leq … \leq\lambda_n$，以及这$n$个特征值所对应的特征向量$w_1, w_2, …, w_n$,那么矩阵A就可以用以下的特征分解表示：</p>
<script type="math/tex; mode=display">
W^{-1}AW=\Lambda \Leftrightarrow A=W\Lambda W^{-1}</script><p>其中，$W$是这$n$个特征向量所组成的$n\times n$维矩阵，而$\Lambda$是将这$n$个特征值作为主对角线的$n\times n$维矩阵。一般情况下，我们会把$W$的这$n$个特征向量标准化，即满足$||w_i||_2=1$，或者$w_i^Tw_i=1$,此时$W$的$n$个特征向量为标准正交基，满足$W^TW=I$，即$W^T=W^{-1}$,也就是说$W$为酉矩阵。这样我们的特征分解表达式可以写成：</p>
<script type="math/tex; mode=display">
A=W\Lambda W^T</script><blockquote>
<p>题外延伸———矩阵压缩：</p>
<p>设$W=(w_1, w_2, w_3,…,w_n)$，则:</p>
<script type="math/tex; mode=display">W^T=\begin{pmatrix}
 w_1^T\\ 
 w_2^T\\ 
 w_3^T\\ 
... \\
w_n^T
\end{pmatrix}</script><p>那么:</p>
<script type="math/tex; mode=display">
A=(w_1, w_2, w_3,...,w_n)\begin{pmatrix}
    \lambda_1&&&\\
    &\lambda_2&&\\
    &&\lambda_3&\\
    &&&...\\
    &&&&\lambda_n
\end{pmatrix}\begin{pmatrix}
 w_1^T\\ 
 w_2^T\\ 
 w_3^T\\ 
... \\
w_n^T
\end{pmatrix}
\\
=\lambda_1w_1w_1^T+\lambda_2w_2w_2^T+\lambda_3w_3w_3^T+...+\lambda_nw_nw_n^T</script><p>假设A为$n\times n$维矩阵，如果正常表示矩阵A共需使用$n^2$个元素，如果将取得的特征值$\lambda_1,\lambda_2,\lambda_3,…,\lambda_n$按从大到小排序，即$\lambda_1\geq\lambda_2\geq\lambda_3\geq…\geq\lambda_n$，则将A的压缩表示为$\lambda_1w_1w_1^T$，即只需要$n+1$个元素。</p>
</blockquote>
<p>注意到要进行特征分解，矩阵A必须为方阵。</p>
<p>那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。</p>
<h3 id="2-SVD推导"><a href="#2-SVD推导" class="headerlink" title="2. SVD推导"></a>2. SVD推导</h3><h4 id="Step1：矩阵分解"><a href="#Step1：矩阵分解" class="headerlink" title="Step1：矩阵分解"></a>Step1：矩阵分解</h4><p>假如$A$为$m\times n$维矩阵，则$A^TA$为对称正定矩阵。</p>
<blockquote>
<p>证明：1)对称性：$(A^TA)^T=A^TA\Rightarrow 对称性$<br>2)正定性： $x^TA^TAx=(Ax)^T(Ax)\geq 0\Rightarrow正定性$</p>
</blockquote>
<p>对于矩阵A，有$(A^TA)v_i=\lambda _iv_i$，其中$\lambda_i$为特征值，$v_i$为特征向量。假定$(v_i, v_j)$是一组正交基，那么有$v_i^T\cdot v_j=0$，那么：</p>
<script type="math/tex; mode=display">
(Av_i, Av_j)=(Av_i)^T\cdot Av_j=v_i^TA^TAv_j=v_i^T\lambda_jv_j=\lambda_jv_i^Tv_j=0</script><p>因此，$Av_i,Av_j$也是一组正交基，根据上述公式可以推导出$(Av_i, Av_i)=\lambda_iv_i^Tv_i=\lambda_i$,从而可以得到：</p>
<script type="math/tex; mode=display">
|Av_i|^2=\lambda_i</script><script type="math/tex; mode=display">
|Av_i|=\sqrt{\lambda_i}</script><p>根据上述公式，有$\frac{Av_i}{|Av_i|}=\frac{1}{\sqrt{\lambda_i}}Av_i$，令$\frac{1}{\sqrt{\lambda_i}}Av_i=u_i$，可得：</p>
<script type="math/tex; mode=display">
Av_i=\sqrt{\lambda_i}u_i=\delta_i u_i</script><p>其中，$\delta_i=\sqrt{\lambda_i}$，进一步推导：</p>
<script type="math/tex; mode=display">
AV=A(v_1,v_2,...,v_n)=(Av_1,Av_2,...,Av_n)=(\delta_1u_1,\delta_2u_2,...,\delta_nu_n)=U\Sigma</script><p>从而得出：</p>
<script type="math/tex; mode=display">A=U\Sigma V^T</script><h4 id="Step2-矩阵计算"><a href="#Step2-矩阵计算" class="headerlink" title="Step2:矩阵计算"></a>Step2:矩阵计算</h4><p>得到矩阵A的表示后，我们应该如何计算向量$U$和$V$呢？继续往下面分析：</p>
<p>首先计算出A的转置$A^T$：$A^T=V\Sigma ^TU^T$</p>
<script type="math/tex; mode=display">
A^TA=V\Sigma^TU^TU\Sigma V^T=V\Sigma^2V^T</script><p>利用上式可以得到，$A^TAv_i=\lambda_iv_i$，只需要求出$A^TA$的特征向量即可得到$V$.</p>
<p>同理可得$AA^T$的值：</p>
<script type="math/tex; mode=display">
AA^T=U\Sigma V^TV\Sigma^TU^T=U\Sigma^2U^T</script><p>可以得到$AA^Tu_i=\lambda_iu_i$，只需要求出$AA^T$的特征向量即可得到$U$.</p>
<blockquote>
<p>题外延伸——-矩阵(图像)压缩：</p>
<p>一个$m\times n$的矩阵A经SVD分解后，可以写成如下形式：</p>
<script type="math/tex; mode=display">
A_{m\times n}=U_{m\times m}\Sigma V^T_{n\times n}=(u_1,u_2,...,u_m)\begin{pmatrix}
    \lambda_1^{\frac{1}{2}}&&\\
    &\lambda_2^{\frac{1}{2}}&\\
    &&...
\end{pmatrix}\begin{pmatrix}
    v_1^T\\
    v_2^T\\
    ...\\
    v_n^T
\end{pmatrix}\\
=\lambda_1^{\frac{1}{2}}u_1v_1^T+\lambda_2^{\frac{1}{2}}u_2v_2^T+...</script><p>假设A为$m\times n$维矩阵，在没有压缩时表示矩阵A共需要$m\times n$个元素。如果将取得的特征值按从大到小排序，即$\lambda<em>1\geq\lambda_2\geq\lambda_3\geq…\geq\lambda</em>{min{m,n}}$，则A的压缩最小压缩表示为$\lambda_1^{\frac{1}{2}}u_1v_1^T$，即需要$m+n+1$个元素。<br>当压缩储存量为$(m+n+1)\times k$时，误差为</p>
<script type="math/tex; mode=display">
error=\frac{(m+n)\times\sum_{i=1}^{k}\lambda_i}{(m+n)\times\sum_{i=1}^{min(m,n)}\lambda_i}=\frac{\sum_{i=1}^{k}\lambda_i}{\sum_{i=1}^{min(m,n)}\lambda_i}</script></blockquote>
<h3 id="例题讲解"><a href="#例题讲解" class="headerlink" title="例题讲解"></a>例题讲解</h3><p>我们举一个简单的例子讲解矩阵时如何进行奇异值分解的。定义矩阵A为：</p>
<script type="math/tex; mode=display">
A=\begin{pmatrix}
0 &1 \\
1&1\\
1&0  
\end{pmatrix}</script><p>首先求出$A^TA、AA^T$：</p>
<script type="math/tex; mode=display">
A^TA=\begin{pmatrix}
    0&1&1\\
    1&1&0
\end{pmatrix}\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}=\begin{pmatrix}
    2&1\\
    1&2
\end{pmatrix}</script><script type="math/tex; mode=display">
AA^T=\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}\begin{pmatrix}
    0&1&1\\
    1&1&0
\end{pmatrix}=\begin{pmatrix}
    1&1&0\\
    1&2&1\\
    0&1&1
\end{pmatrix}</script><p>进而求出$A^TA$的特征值和特征向量：</p>
<script type="math/tex; mode=display">
\lambda_1=3;v_1=\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix};
\lambda_2=1;v_2=\begin{pmatrix}
    -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix}</script><p>接着求出$AA^T$的特征值和特征向量：</p>
<script type="math/tex; mode=display">
\lambda_1=3;u_1=\begin{pmatrix}
    \frac{1}{\sqrt{6}}\\
    \frac{2}{\sqrt{6}}\\
    \frac{1}{\sqrt{6}}
\end{pmatrix};
\lambda_2=1;u_2=\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    0\\
    -\frac{1}{\sqrt{2}}
\end{pmatrix};\lambda_3=0;u_3=\begin{pmatrix}
    \frac{1}{\sqrt{3}}\\
    -\frac{1}{\sqrt{3}}\\
    \frac{1}{\sqrt{3}}
\end{pmatrix}</script><p>利用$Av_i=\delta_iu_i,i=1,2$求奇异值：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix}=\delta_1\begin{pmatrix}
    \frac{1}{\sqrt{6}}\\
    \frac{2}{\sqrt{6}}\\
    \frac{1}{\sqrt{6}}
\end{pmatrix} \Rightarrow\delta_1=\sqrt{3}</script><script type="math/tex; mode=display">
\begin{pmatrix}
    0&1\\
    1&1\\
    1&0
\end{pmatrix}\begin{pmatrix}
    -\frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}
\end{pmatrix}=\delta_2\begin{pmatrix}
    \frac{1}{\sqrt{2}}\\
    0\\
    -\frac{1}{\sqrt{2}}
\end{pmatrix} \Rightarrow\delta_2=1</script><p>也可以用$\delta_i=\sqrt{\lambda_i}$直接求出奇异值为$\sqrt{3}$和$1$.</p>
<p>最终得到矩阵A的奇异值分解为：</p>
<script type="math/tex; mode=display">
A=U\Sigma V^T=\begin{pmatrix}
    \frac{1}{\sqrt{6}}  &\frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{3}} \\
    \frac{2}{\sqrt{6}}  &0  & -\frac{1}{\sqrt{3}} \\
    \frac{1}{\sqrt{6}}  &-\frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{3}} 
\end{pmatrix}\begin{pmatrix}
    \sqrt{3}  & 0\\
    0  & 1\\
    0  & 0
\end{pmatrix}\begin{pmatrix}
    \frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{2}} \\
    -\frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{2}}
\end{pmatrix}</script><h3 id="SVD的一些性质"><a href="#SVD的一些性质" class="headerlink" title="SVD的一些性质"></a>SVD的一些性质</h3><p>对于奇异值，他跟我们特征分解中的特征值类似，在奇艺置矩阵中也是按照从大到小排列，而且奇异值的减少特别快，在很多情况下，前10\%甚至1\%的奇异值就占了全部的奇异值之和的99\%以上的比例。也就是说，我们也可以用最大的k个奇异值和对应的左右奇异向量来近似描述矩阵(与前面描述的题外延伸之矩阵压缩类似)，由于这个重要的性质，SVD也可以用于PCA降维，来做数据压缩和去噪，也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需要来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
<blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/29846048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29846048</a> </p>
<p>参考：<a href="https://www.csuldw.com/2017/03/09/2017-03-09-svd/" target="_blank" rel="noopener">https://www.csuldw.com/2017/03/09/2017-03-09-svd/</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/" class="post-title-link" itemprop="url">反向传播算法推导</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:21" itemprop="dateModified" datetime="2020-06-20T20:12:21+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="反向传播算法详细推导过程"><a href="#反向传播算法详细推导过程" class="headerlink" title="反向传播算法详细推导过程"></a><strong>反向传播算法详细推导过程</strong></h1><h2 id="1-前向传播"><a href="#1-前向传播" class="headerlink" title="1. 前向传播"></a><strong>1. 前向传播</strong></h2><p><img src="./images/feedfoward.jpg" alt></p>
<p>假设$X$为$N\times m$的矩阵（其中，$N$为样本个数（batch size），$m$为特征维数）</p>
<p>$h<em>1$与$Z_1$的维数为$m_1 \rightarrow W_1$为$m\times m_1$的矩阵，$b_1 \in \mathbb{R}^{m_1},\$<br>$h_2$与$Z_2$的维数为$m_2 \rightarrow W_2$为$m_1\times m_2$的矩阵，$b_2 \in \mathbb{R}^{m_2},\$<br>${\vdots}\$<br>$h_L$与$Z_L$的维数为$m_L \rightarrow W_L$为$m</em>{L-1}\times m_L$的矩阵，$b_L \in \mathbb{R}^{m_L}$</p>
<h3 id="前向算法："><a href="#前向算法：" class="headerlink" title="前向算法："></a>前向算法：</h3><script type="math/tex; mode=display">
\begin{array}{l}{h_{1}=x W_{1}+\tilde{b}_{1}, Z_{1}=f_{1}\left(h_{1}\right), \tilde{b}_{1}}为b_1^T沿着行方向扩展成N行 \\ {h_{2}=Z_{1} W_{2}+\tilde{b}_{2}, Z_{2}=f_{2}\left(h_{2}\right)} \\ {\vdots} \\ {h_{L}=Z_{L-1} W_{L}+\tilde{b}_{L}, Z_{L}=f_{L}\left(h_{L}\right)} \\ {\text { out }=Z_{L} W_{L+1}+\tilde{b}_{L+1}}\end{array}</script><p>假设输出为$n$维，则$out$为大小为$N\times n$的矩阵，根据MSE或CE准则可以求得$\frac{\partial J}{\partial out}$，对于回归问题与分类问题，$\frac{\partial J}{\partial out}$的求解方法如下：</p>
<table align="center">
    <tr>
        <td><center><img height="200" src="./images/回归问题.jpg">回归问题</center></td>
        <td><center><img height="200" src="./images/分类问题.jpg">分类问题</center></td>
    </tr>
</table>

<ul>
<li>对于回归问题，对out直接计算损失，损失函数为MSE。 损失：$J=\frac{1}{2N}\sum_{i=1}^{N}||y_i-\tilde{y_i}||^2$<script type="math/tex; mode=display">
\begin{aligned}
    \frac{\partial J}{\partial y_i}&=\frac{1}{2N}\sum_{i=1}^{N}(y_i-\tilde{y_i})\times 2 \\
    &=\frac{1}{N}\sum_{i=1}^{N}(y_i-\tilde{y_i})
\end{aligned}</script></li>
<li>对于分类问题，out后接softmax进行分类，然后使用CE(cross entropy)计算loss.$S<em>k=\frac{e^{y_k}}{\sum</em>{i=1}^{n}e^{y_i}}$一个样本对应的网络的输出$S(s_1,s_2,…,s_n)$是一个概率分布，而这个样本的标注$\tilde{S}$一般为$(0,0,…,1,0,0,…,0)$，也可以看做一个概率分布（硬分布）。cross entropy可以看成是$S$与$\tilde{S}$之间的KL距离：<script type="math/tex; mode=display">D(\tilde{S}||S)=\Sigma\tilde{S}\log\frac{\tilde{S}}{S}</script><ul>
<li>假设$\tilde{S}=(0,0,…,1,0,0,…,0)$，其中1为第$k$个元素(索引从0开始)，令$S=(s<em>0,s_1,…,s_k,…,s</em>{n-1})$.<br>损失：<script type="math/tex; mode=display">
\begin{aligned}
  J=D(\tilde{S}||S)&=1\times \log\frac{1}{s_k}\\&=-\log s_k \quad(CE损失函数,可看做目标类别概率最大)\\
  &=-\log\frac{e^{y_k}}{\sum_{i=0}^{n-1}e^{y_i}}
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
 &\frac{\partial J}{\partial y_m}=\frac{\partial J}{\partial y_m}(\log \sum_{i=0}^{n-1}e^{y_i}-y_k)=\frac{e^{y_m}}{\sum_{i=0}^{n-1}e^{y_i}}-\delta(m=k)=s_m-\delta(m=k) \\
&写成向量形式为:\frac{\partial J}{\partial y}=S-\tilde{S}
\end{aligned}</script></li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>KL距离（相对熵）</strong>：是Kullback-Leibler Divergence的简称，也叫相对熵(Relative Entropy).它衡量的是相同事件空间里的两个概率分布的差异情况。其物理意义是：在相同事件空间里，概率分布$P(x)$对应的每个事件，若用概率分布$Q(x)$编码时，平均每个基本事件(符号)编码长度增加了多少比特。我们用$D(P||Q)$表示KL距离，计算公式如下：</p>
<script type="math/tex; mode=display">
D(P||Q)=\sum_{x\in X}P(x)\log\frac{P(x)}{Q(x)}</script><p>当两个概率分布完全相同时，即$P(X)=Q(X)$,其相对熵为0.</p>
<h2 id="2-反向传播"><a href="#2-反向传播" class="headerlink" title="2.反向传播"></a><strong>2.反向传播</strong></h2><p>$\text { out }=Z<em>{L} W</em>{L+1}+\tilde{b}<em>{L+1}$，为了便于详细说明反向传播算法，假设$Z_L$为$2\times 3$的向量，$W</em>{L+1}$为$3\times 2$的向量：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{Z_{L}=\left(\begin{array}{ccc}{z_{11}} & {z_{12}} & {z_{13}} \\ {z_{21}} & {z_{22}} & {z_{23}}\end{array}\right)_{2 \times 3}, W_{L+1}=\left(\begin{array}{cc}{w_{11}} & {w_{12}} \\ {w_{21}} & {w_{22}} \\ {w_{31}} & {w_{32}}\end{array}\right)_{3 \times 2} \tilde{b}_{L+1}=\left(\begin{array}{cc}{b_{1}} & {b_{2}} \\ {b_{1}} & {b_{2}}\end{array}\right)_{2 \times 2}, \text { out }=\left(\begin{array}{cc}{o_{11}} & {o_{12}} \\ {o_{21}} & {o_{22}}\end{array}\right)} \\ \Rightarrow  {Z_{L}W_{L+1}+\tilde{b}_{L+1}=\left(\begin{array}{cc}{z_{11} w_{11}+z_{12} w_{21}+z_{13} w_{31}+b_1} & {z_{11} w_{12}+z_{12} w_{22}+z_{13} w_{32}+b_2} \\ {z_{21} w_{11}+z_{22} w_{21}+z_{23} w_{31}+b_1} & {z_{21} w_{12}+z_{22} w_{22}+z_{23} w_{32}+b_2}\end{array}\right)=\text{out}.}\end{array}</script><p>所以，</p>
<script type="math/tex; mode=display">
\begin{array}{l}{o_{11}=z_{11} w_{11}+z_{12} w_{21}+z_{13} w_{31}+b_{1}} \\ {o_{12}=z_{11} w_{12}+z_{12} w_{22}+z_{13} w_{32}+b_{2}} \\ {o_{21}=z_{21} w_{11}+z_{22} w_{21}+z_{23} w_{31}+b_{1}} \\ {o_{22}=z_{21} w_{12}+z_{22} w_{22}+z_{23} w_{32}+b_{2}}\end{array}</script><h4 id="1-损失-J-对-W-的导数："><a href="#1-损失-J-对-W-的导数：" class="headerlink" title="1) 损失$J$对$W$的导数："></a><strong>1) 损失$J$对$W$的导数：</strong></h4><script type="math/tex; mode=display">
\begin{aligned} \frac{\partial J}{\partial w_{11}} &=\frac{\partial J}{\partial o_{11}} z_{11}+\frac{\partial J}{\partial o_{21}} z_{21}, \frac{\partial J}{\partial w_{12}}=\frac{\partial J}{\partial o_{12}} z_{11}+\frac{\partial J}{\partial o_{22}} z_{21} \\ \frac{\partial J}{\partial w_{21}} &=\frac{\partial J}{\partial o_{11}} z_{12}+\frac{\partial J}{\partial o_{21}} z_{22}, \frac{\partial J}{\partial w_{22}}=\frac{\partial J}{\partial o_{12}} z_{12}+\frac{\partial J}{\partial o_{22}} z_{22} \\ \frac{\partial J}{\partial w_{31}} &=\frac{\partial J}{\partial o_{11}} z_{13}+\frac{\partial J}{\partial o_{21}} z_{23}, \frac{\partial J}{\partial w_{32}}=\frac{\partial J}{\partial o_{12}} z_{13}+\frac{\partial J}{\partial o_{22}} z_{23} \end{aligned}</script><script type="math/tex; mode=display">
\Rightarrow
\left(\begin{array}{cc}{\frac{\partial J}{\partial w_{11}}} & {\frac{\partial J}{\partial w_{12}}} \\ {\frac{\partial J}{\partial w_{21}}} & {\frac{\partial J}{\partial w_{22}}} \\ {\frac{\partial J}{\partial w_{31}}} & {\frac{\partial J}{\partial w_{32}}}\end{array}\right)=\left(\begin{array}{cc}{z_{11}} & {z_{21}} \\ {z_{12}} & {z_{22}} \\ {z_{13}} & {z_{23}}\end{array}\right)\left(\begin{array}{cc}{\frac{\partial J}{\partial o_{11}}} & {\frac{\partial J}{\partial o_{12}}} \\ {\frac{\partial J}{\partial o_{21}}} & {\frac{\partial J}{\partial o_{22}}}\end{array}\right)</script><p>即，$\frac{\partial J}{\partial W_{L+1}}=Z_L^T\frac{\partial J}{\partial out}$</p>
</blockquote>
<h4 id="2-损失对偏置b的导数等于将-frac-partial-J-partial-out-的每一列加起来："><a href="#2-损失对偏置b的导数等于将-frac-partial-J-partial-out-的每一列加起来：" class="headerlink" title="2) 损失对偏置b的导数等于将$\frac{\partial J}{\partial out}$的每一列加起来："></a><strong>2) 损失对偏置b的导数等于将$\frac{\partial J}{\partial out}$的每一列加起来：</strong></h4><script type="math/tex; mode=display">
\left\{\begin{array}{l}{\frac{\partial J}{\partial b_{1}}=\frac{\partial J}{\partial o_{11}}+\frac{\partial J}{\partial o_{21}}} \\ {\frac{\partial J}{\partial b_{2}}=\frac{\partial J}{\partial o_{12}}+\frac{\partial J}{\partial o_{22}}}\end{array} \Rightarrow\left(\frac{\partial J}{\partial b_{L+1}}\right)^{T}=\left(\frac{\partial J}{\partial b_{1}} \quad \frac{\partial J}{\partial b_{2}}\right)=\left(\frac{\partial J}{\partial o_{11}}+\frac{\partial J}{\partial o_{21}} \quad \frac{\partial J}{\partial o_{12}}+\frac{\partial J}{\partial o_{22}}\right)\right.</script><h4 id="3-损失-J-对-Z-的导数："><a href="#3-损失-J-对-Z-的导数：" class="headerlink" title="3) 损失$J$对$Z$的导数："></a><strong>3) 损失$J$对$Z$的导数：</strong></h4><script type="math/tex; mode=display">
\begin{aligned} \frac{\partial J}{\partial z_{11}} &=\frac{\partial J}{\partial o_{11}} w_{11}+\frac{\partial J}{\partial o_{12}} w_{12} ; \frac{\partial J}{\partial z_{12}}=\frac{\partial J}{\partial o_{11}} w_{21}+\frac{\partial J}{\partial o_{12}} w_{22} ; \frac{\partial J}{\partial z_{13}}=\frac{\partial J}{\partial o_{11}} w_{31}+\frac{\partial J}{\partial o_{12}} w_{32} \\ \frac{\partial J}{\partial z_{21}} &=\frac{\partial J}{\partial o_{21}} w_{11}+\frac{\partial J}{\partial o_{22}} w_{12} ; \frac{\partial J}{\partial z_{22}}=\frac{\partial J}{\partial o_{21}} w_{21}+\frac{\partial J}{\partial o_{12}} w_{22} ; \frac{\partial J}{\partial z_{23}}=\frac{\partial J}{\partial o_{21}} w_{31}+\frac{\partial J}{\partial o_{22}} w_{32} \end{aligned}</script><p>即，</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}{\frac{\partial J}{\partial z_{11}}} & {\frac{\partial J}{\partial z_{12}}} & {\frac{\partial J}{\partial z_{13}}} \\ {\frac{\partial J}{\partial z_{21}}} & {\frac{\partial J}{\partial z_{22}}} & {\frac{\partial J}{\partial z_{23}}}\end{array}\right)=\left(\begin{array}{cc}{\frac{\partial J}{\partial o_{11}}} & {\frac{\partial J}{\partial o_{12}}} \\ {\frac{\partial J}{\partial \theta_{21}}} & {\frac{\partial J}{\partial o_{22}}}\end{array}\right)\left(\begin{array}{ccc}{w_{11}} & {w_{21}} & {w_{31}} \\ {w_{12}} & {w_{22}} & {w_{32}}\end{array}\right)</script><p>$\Rightarrow \frac{\partial J}{\partial Z<em>{L}}=\frac{\partial J}{\partial out}W</em>{L+1}^T$</p>
<h4 id="4-损失-J-对-h-的导数："><a href="#4-损失-J-对-h-的导数：" class="headerlink" title="4) 损失$J$对$h$的导数："></a><strong>4) 损失$J$对$h$的导数：</strong></h4><script type="math/tex; mode=display">Z_L = f_L(h_L)</script><ul>
<li>$f_L$为sigmoid时，$Z_L=\frac{1}{1+e^{-h_L}}$.<script type="math/tex; mode=display">
\begin{array}{l}{\frac{\partial J}{\partial h_{L}}=\frac{\partial J}{\partial Z_{L}} \frac{d z_{L}}{d h_{L}}=\frac{\partial J}{\partial Z_{L}} \frac{e^{-h L}}{\left(1+e^{-h_{L}}\right)^{2}}=\frac{\partial J}{\partial Z_{L}} \frac{1}{1+e^{-h_{L}}} \frac{e^{-h_{L}}}{1+e^{-h_{L}}}} \\ {=\frac{\partial J}{\partial Z_{L}} Z_{L}\left(1-Z_{L}\right)}\end{array}</script></li>
<li>$f<em>L$为tanh时，${Z</em>{L}=\frac{e^{h<em>{L}}-e^{-h</em>{L}}}{e^{h<em>{L}}+e^{-h</em>{L}}}}$.<script type="math/tex; mode=display">
\begin{array}{l} {\frac{\partial J}{\partial h_{L}}=\frac{\partial J}{\partial Z_{L}} \frac{d Z_{L}}{d h_{L}}=\frac{\partial J}{\partial Z_{L}} \frac{4}{\left(e^{h_{L}}+e^{-h_{L}}\right)^{2}}=\frac{\partial J}{\partial Z_{L}}\left[1-\left(\frac{e^{h_{L}}-e^{-h_{L}}}{e^{h_{L}}+e^{-h_{L}}}\right)^{2}\right]} \\ {=\frac{\partial J}{\partial z_{L}}\left[1-z_{L}^{2}\right]}\end{array}</script></li>
<li>$f_L$为relu时，$Z_L=relu(h_L)=\left{\begin{matrix}<br>0,&amp;h_L\leq 0 \<br>h_L,&amp;h_L &gt; 0<br>\end{matrix}\right.$.<script type="math/tex; mode=display">
\begin{array}{l}
  \frac{\partial J}{\partial h_L}=\frac{\partial J}{\partial Z_L}\frac{\partial Z_L}{\partial h_L}=\left\{\begin{matrix}
0,&h_L\leq 0 \\ 
\frac{\partial J}{\partial Z_L},&h_L > 0 
\end{matrix}\right.
\end{array}</script></li>
</ul>
<h2 id="3-梯度更新"><a href="#3-梯度更新" class="headerlink" title="3. 梯度更新"></a><strong>3. 梯度更新</strong></h2><p>对于不同算法 ，梯度更新方式如下：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial out} \Rightarrow \left \{\begin{matrix}
    \frac{\partial J}{\partial W_{L+1}}=Z_L^T\frac{\partial J}{\partial out} \\
    \frac{\partial J}{\partial Z_{L}}=\frac{\partial J}{\partial out}W_{L+1}^T \\
    \left(\frac{\partial J}{\partial b}\right)^{T}=SumCol(\frac{\partial J}{\partial out}) \\
    W_{L+1}^{t+1} = W_{L+1}^t-\eta \frac{\partial J}{\partial W_{L+1}} \\
    b_{L+1}^{t+1} = b_{L+1}^t-\eta \frac{\partial J}{\partial b_{L+1}}
\end{matrix} \right. \Rightarrow \frac{\partial J}{\partial h_L}=\frac{\partial J}{\partial Z_L}\frac{\partial Z_L}{\partial h_L} \Rightarrow \left \{\begin{matrix} 
    \frac{\partial J}{\partial W_{L}}=Z_{L-1}^T\frac{\partial J}{\partial h_L} \\
    \frac{\partial J}{\partial Z_{L-1}}=\frac{\partial J}{\partial h_L}W_{L}^T \\
    \vdots \\
    \vdots 
\end{matrix}\right. \Rightarrow \cdots</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chennan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenzhongyao's website">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">深度学习中的模型优化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-18 23:02:52" itemprop="dateCreated datePublished" datetime="2020-06-18T23:02:52+08:00">2020-06-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-20 20:12:21" itemprop="dateModified" datetime="2020-06-20T20:12:21+08:00">2020-06-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习中的模型优化"><a href="#深度学习中的模型优化" class="headerlink" title="深度学习中的模型优化"></a>深度学习中的模型优化</h1><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="Batch-Gradient-Descent-BGD"><a href="#Batch-Gradient-Descent-BGD" class="headerlink" title="Batch Gradient Descent(BGD)"></a>Batch Gradient Descent(BGD)</h3><p>BGD在训练中,每一步迭代都是用训练集中的所有数据,也就是说,利用现有参数对训练集中的每一个输入生成一个估计输出,然后跟实际输出比较,统计所有误差,求平均以后得到平均误差,并以此作为更新参数的依据.</p>
<ul>
<li><strong>优点:</strong> 由于每一步都利用了训练集中的所有数据,因此当损失函数达到最小值以后,能够保证此时计算出的梯度为0,换句话说,就是能够收敛(曲线比较平滑),因此,使用BGD时不需要逐渐降低学习率.</li>
<li><strong>缺点:</strong> 由于每一步都要用到所有训练数据,因此随着数据集的增大,运行速度会越来越慢.<h3 id="SGD与MBGD"><a href="#SGD与MBGD" class="headerlink" title="SGD与MBGD"></a>SGD与MBGD</h3>MBGD是指在训练中,每次使用小批量(一个小批量训练m个样本)的随机采样进行梯度下降.训练方法与BGD一样,只是BGD最后对训练集的所有样本取平均,而MBGD只对小批量的m个样本取平均.SGD是指在训练中每次仅使用一个样本.MBGD与SGD统称为SGD.因为小批量不能代表整个训练集,使得梯度估计引入噪声源,因此SGD并不是每次迭代都向着整体最优化方向.虽然SGD包含一定的随机性(表现为损失函数的震荡),但是从期望来看,它是等于正确的导数的(表现为损失函数有减小的趋势).</li>
<li><strong>优点:</strong> 训练速度比较快</li>
<li><strong>缺点:</strong> 在样本数量较大的情况下,可能只用到了其中一部分数据就完成了训练,得到的只是局部最优解.另外,小批量样本的噪声较大,所以每次执行梯度下降,并不一定总是朝着最优的方向前进.<br><img src="./images/SGD与BGD.png" alt><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><script type="math/tex; mode=display">\left \{
\begin{array}{l}{\boldsymbol{v}_{t}=-\eta \triangledown_\theta J\left(\boldsymbol{\theta}_{t}\right) } \\ {\boldsymbol{\theta}_{t} =\boldsymbol{\theta}_{t-1}+\boldsymbol{v}_{t}}\end{array} \right.</script>其中，$\eta$代表学习率，$\theta<em>t$表示$t$时刻的参数，$\triangledown </em>\theta J(\theta_t)$代表参数$\theta$在$t$时刻的导数，$v_t$代表参数的更新速度。<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2>在训练中，采取的策略与SGD一样，不同的是学习率的更新方式。动量的参数更新方式为：<script type="math/tex; mode=display">\left \{
\begin{array}{l}{\boldsymbol{v}_{t}=-\eta \triangledown_\theta J\left(\boldsymbol{\theta}_{t}\right)+\alpha \boldsymbol{v}_{t-1}} \\ {\boldsymbol{\theta}_{t+1} =\boldsymbol{\theta}_{t}+\boldsymbol{v}_{t}}\end{array} \right.</script>$\alpha$一般取值0.9.</li>
</ul>
<p>直观理解为：<br><img src="./images/momentum.jpg" alt><br>动量方法旨在加速学习（加快梯度的下降速度），特别是处理高曲率、小但一致的梯度，或是带噪声的梯度。动量算法累积了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。</p>
<table align="center">
    <tr>
        <td><center><img height="200" width="300" src="./images/momentum.gif"></center></td>
    </tr>
</table>

<p>动量SGD算法引入$\alpha \boldsymbol{v}_{t-1}$使每一次的参数更新方向不仅仅取决于当前位置的梯度，还受到上一次参数更新方向的影响（如上图所示）。在某一维度上，当梯度方向不变时，更新速度变快，当梯度方向有所改变时，更新速度变慢，从而加快收敛速度，减少震荡。</p>
<p>带有动量的SGD的优点：</p>
<ul>
<li>加快收敛速度</li>
<li>抑制梯度下降时上下震荡的情况</li>
<li>通过局部极小点</li>
</ul>
<blockquote>
<p>分析：假设任意时刻参数的梯度均为$g<em>t=\triangledown</em>\theta J\left(\boldsymbol{\theta}_{t}\right)=g_0$，则使用SGD时,$t$时刻的梯度$g^{SGD}_t=g_0$,Momentum算法的梯度$g^{mom}_t=(\alpha^{t-1}+\alpha^{t-2}+…+\alpha+1)g_0=\frac{1-\alpha^{t}}{1-\alpha}g_0$.当$t\rightarrow +\infty$，因$\alpha&lt;1$,所以$\alpha^t\rightarrow 0$,所以$g_t^{mom}\rightarrow \frac{1}{1-\alpha}g_0$,当$\alpha=0.9$时，Momentum更新速度是SGD的10倍</p>
</blockquote>
<h2 id="Nesterov-NAG"><a href="#Nesterov-NAG" class="headerlink" title="Nesterov(NAG)"></a>Nesterov(NAG)</h2><p>Nesterov动量是Momentum的变种，即在计算参数梯度之前，前瞻一步，超前一个动量单位处：$\theta<em>t + \gamma v</em>{t-1}$,Nesterov动量可以理解为往Momentum动量中加入了一个校正因子。参数更新公式为：</p>
<script type="math/tex; mode=display">\left \{
\begin{array}{l}{\boldsymbol{v}_{t}=-\eta \triangledown_\theta J\left(\boldsymbol{\theta}_{t}+\gamma \boldsymbol{v}_{t-1}\right)+\alpha \boldsymbol{v}_{t-1}} \\ {\boldsymbol{\theta}_{t+1} =\boldsymbol{\theta}_{t}+\boldsymbol{v}_{t}}\end{array} \right.</script><p>Momentum动量与Nesterov动量的对比如下图所示，其中$\eta \triangledown<em>1$代表A节点 ($\theta_t$)的梯度，$\eta \triangledown_2$代表B节点($\theta</em>{t}+\gamma \boldsymbol{v}<em>{t-1}$的梯度)，灰色实线代表$t-1$时刻的速度$\alpha v</em>{t-1}$.</p>
<table align="center">
    <tr>
        <td><center><img height="230" width="250" src="./images/nesterov.png"></center></td>
        <td><center><img height="230" width="300" src="./images/nesterov.bmp"></center></td>
    </tr>
</table>

<blockquote>
<p>注意：图中的$\eta \triangledown_1以及\eta \triangledown_2$应该为$-\eta \triangledown_1$、$-\eta \triangledown_2$因为梯度方向是增长速度最快的方向，而图中所示为梯度的反方向，所以应该为$-\eta \triangledown_1$、$-\eta \triangledown_2$.</p>
</blockquote>
<p>Nesterov动量相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似，由于令了二阶导的信息，Nesterov动量算法才会比Momentum具有更快的收敛速度。</p>
<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>AdaGrad其实是对学习率进行了约束，AdaGrad独立地适应所有模型参数的学习率，缩放每个参数反比于其它所有梯度历史平方值总和的平方根。损失较大偏导的参数相应地拥有一个快速下降的学习率，而较小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。参数更新公式为：</p>
<script type="math/tex; mode=display">\left \{
\begin{array}{l}{
    g_t=\triangledown_\theta J(\theta_t)}\\
    {n_t=n_{t-1}+g_t^2} \\
    {\boldsymbol{v}_{t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}g_t } \\ {\boldsymbol{\theta}_{t+1} =\boldsymbol{\theta}_{t}+\boldsymbol{v}_{t}}\end{array} \right.</script><p>其中，$\epsilon$是个很小的数，用来保证分母非0。对$𝑔_𝑡$从1到t进行一个递推形成一个约束项regularizer——-$\frac{1}{\sqrt{n_t+\epsilon}}$。</p>
<p><strong>优点：</strong><br>前期$g_t$较小的时候， $\frac{1}{\sqrt{n_t+\epsilon}}$较大，梯度更新较大，可以解决SGD中学习率一直不变的问题;后期$g_t$较大的时候，$\frac{1}{\sqrt{n_t+\epsilon}}$较小，能够约束梯度.适合处理稀疏梯度.</p>
<p><strong>缺点：</strong><br>由公式可以看出，AdaGrad依赖于人工设置一个全局学习率𝜂，当$\eta$设置过大时，使regularizer过于敏感，对梯度的调节太大。在中后期，分母上梯度平方的累加将会越来越大，gradient→0，网络的更新能力会越来越弱，学习率会变的极小，使得训练提前结束。为了解决这样的问题，又提出了Adadelta算法。</p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adadelta是对AdaGrad的扩展，AgaGrad会累加所有历史梯度的平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。参数更新方式为：</p>
<script type="math/tex; mode=display">\left \{
\begin{array}{l}{
    g_t=\triangledown_\theta J(\theta_t)}\\
    {n_t=\gamma n_{t-1}+(1-\gamma)g_t^2}\\
    {\boldsymbol{v}_{t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}g_t } \\ {\boldsymbol{\theta}_{t+1} =\boldsymbol{\theta}_{t}+\boldsymbol{v}_{t}}\end{array} \right.</script><p>但是，此时Adadelta其实仍然依赖于全局学习率，因此，又做了一些处理，新的参数更新方式为：</p>
<script type="math/tex; mode=display">\left \{
\begin{array}{l}{
    g_t=\triangledown_\theta J(\theta_t)}\\
    {E|g^2|_t=\rho \times E|g^2|_{t-1}+(1-\rho)\times g_t^2}\\
    {v_t=-\frac{\sqrt{\sum_{r=1}^{t-1}}v_r}{\sqrt{E|t^2|_t+\epsilon}}g_t}\\
    {\boldsymbol{\theta}_{t+1} =\boldsymbol{\theta}_{t}+\boldsymbol{v}_{t}}\end{array} \right.</script><p>新的参数更新方式，不依赖于全局学习率，并且，训练初中期，加速效果不错，很快；训练后期，反复在局部最小值附近抖动。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop可以算作Adadelta的一个特例：当$\rho=0.5$时，$E|g^2|<em>t=\rho \times E|g^2|</em>{t-1}+(1-\rho)\times g_t^2$就变为了求梯度平方和的平均数。</p>
<p>如果再求根的话，就变成了RMS(均方根)：$RMS|g|_t=\sqrt{E|g^2|_t+\epsilon}$。RMSprop的参数更新方式为：</p>
<script type="math/tex; mode=display">\left \{
\begin{array}{l}{
    g_t=\triangledown_\theta J(\theta_t)}\\
    {E|g^2|_t=\rho \times E|g^2|_{t-1}+(1-\rho)\times g_t^2} \\
    {RMS|g|_t=\sqrt{E|g^2|_t+\epsilon}}\\
    {\boldsymbol{v}_{t}=-\frac{\eta}{RMS|g|_t }g_t} \\ {\boldsymbol{\theta}_{t+1} =\boldsymbol{\theta}_{t}+\boldsymbol{v}_{t}}\end{array} \right.</script><p><strong>特点：</strong> (1)RMSprop依然依赖于全局学习率;(2)RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间;(3)适合处理非平稳目标 - 对于RNN效果很好</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam优化器结合了AdaGrad与RMSProp两种算法的优点。对梯度的一阶距估计$m_t$（即梯度的均值）和二阶距估计$n_t$（即梯度的未中心化的方差）进行综合考虑，计算出更新步长。更新方式为：</p>
<script type="math/tex; mode=display">\left \{
\begin{array}{l}{
    g_t=\triangledown_\theta J(\theta_t)}\\
    {m_t=\beta_1m_{t-1}+(1-\beta_1)g_t}\\
    {v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2}\\
    {\hat{m_t}=\frac{m_t}{1-\beta_1^t}}\\
    {\hat{v}_t=\frac{v_t}{1-\beta_2^t}}\\
    {\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}-\frac{\eta}{\sqrt{\hat{v}_t+\epsilon}}\hat{m}_t}\\
\end{array} \right.</script><blockquote>
<p>注：所有的$t$均表示$t$时刻。$m_t$、$n_t$分别是梯度的一阶距估计和二阶距估计，可以看做是对期望$E|g_t|、E|g_t^2|$的估计；$\hat{m}_t、\hat{n}_t$是对$m_t$、$n_t$的校正，这样可以近似为对期望的无偏估计。</p>
</blockquote>
<p><strong>优点：</strong></p>
<ol>
<li>实现简单，计算高效，对内存需求少</li>
<li>参数的更新不受梯度的伸缩变换影响</li>
<li>超参数具有很好的解释性，且通常无需调整或仅需很少的微调</li>
<li>更新的步长能够被限制在大致的范围内（初始学习率）</li>
<li>能自然地实现步长退火过程（自动调整学习率）</li>
<li>很适合应用于大规模的数据及参数的场景</li>
<li>适用于不稳定目标函数</li>
<li>适用于梯度稀疏或梯度存在很大噪声的问题</li>
</ol>
<blockquote>
<p>参考资料：</p>
<p><a href="https://zhuanlan.zhihu.com/p/73264637" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73264637</a> </p>
<p><a href="https://zhuanlan.zhihu.com/p/60088231" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60088231</a></p>
<p><a href="https://blog.csdn.net/u012759136/article/details/52302426" target="_blank" rel="noopener">https://blog.csdn.net/u012759136/article/details/52302426</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">chennan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chennan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  











<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
