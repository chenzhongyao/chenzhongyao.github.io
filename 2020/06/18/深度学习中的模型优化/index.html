<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>深度学习中的模型优化 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="深度学习中的模型优化随机梯度下降Batch Gradient Descent(BGD)BGD在训练中,每一步迭代都是用训练集中的所有数据,也就是说,利用现有参数对训练集中的每一个输入生成一个估计输出,然后跟实际输出比较,统计所有误差,求平均以后得到平均误差,并以此作为更新参数的依据.  优点: 由于每一步都利用了训练集中的所有数据,因此当损失函数达到最小值以后,能够保证此时计算出的梯度为0,换句话">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习中的模型优化">
<meta property="og:url" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="深度学习中的模型优化随机梯度下降Batch Gradient Descent(BGD)BGD在训练中,每一步迭代都是用训练集中的所有数据,也就是说,利用现有参数对训练集中的每一个输入生成一个估计输出,然后跟实际输出比较,统计所有误差,求平均以后得到平均误差,并以此作为更新参数的依据.  优点: 由于每一步都利用了训练集中的所有数据,因此当损失函数达到最小值以后,能够保证此时计算出的梯度为0,换句话">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/images/SGD%E4%B8%8EBGD.png">
<meta property="og:image" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/images/momentum.jpg">
<meta property="og:image" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/images/momentum.gif">
<meta property="og:image" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/images/nesterov.png">
<meta property="og:image" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/images/nesterov.bmp">
<meta property="article:published_time" content="2020-06-18T13:41:36.881Z">
<meta property="article:modified_time" content="2019-09-03T17:45:25.476Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/images/SGD%E4%B8%8EBGD.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://chenzhongyao.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-深度学习中的模型优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" class="article-date">
  <time datetime="2020-06-18T13:41:36.881Z" itemprop="datePublished">2020-06-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习中的模型优化
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="深度学习中的模型优化"><a href="#深度学习中的模型优化" class="headerlink" title="深度学习中的模型优化"></a>深度学习中的模型优化</h1><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="Batch-Gradient-Descent-BGD"><a href="#Batch-Gradient-Descent-BGD" class="headerlink" title="Batch Gradient Descent(BGD)"></a>Batch Gradient Descent(BGD)</h3><p>BGD在训练中,每一步迭代都是用训练集中的所有数据,也就是说,利用现有参数对训练集中的每一个输入生成一个估计输出,然后跟实际输出比较,统计所有误差,求平均以后得到平均误差,并以此作为更新参数的依据.</p>
<ul>
<li><strong>优点:</strong> 由于每一步都利用了训练集中的所有数据,因此当损失函数达到最小值以后,能够保证此时计算出的梯度为0,换句话说,就是能够收敛(曲线比较平滑),因此,使用BGD时不需要逐渐降低学习率.</li>
<li><strong>缺点:</strong> 由于每一步都要用到所有训练数据,因此随着数据集的增大,运行速度会越来越慢.<h3 id="SGD与MBGD"><a href="#SGD与MBGD" class="headerlink" title="SGD与MBGD"></a>SGD与MBGD</h3>MBGD是指在训练中,每次使用小批量(一个小批量训练m个样本)的随机采样进行梯度下降.训练方法与BGD一样,只是BGD最后对训练集的所有样本取平均,而MBGD只对小批量的m个样本取平均.SGD是指在训练中每次仅使用一个样本.MBGD与SGD统称为SGD.因为小批量不能代表整个训练集,使得梯度估计引入噪声源,因此SGD并不是每次迭代都向着整体最优化方向.虽然SGD包含一定的随机性(表现为损失函数的震荡),但是从期望来看,它是等于正确的导数的(表现为损失函数有减小的趋势).</li>
<li><strong>优点:</strong> 训练速度比较快</li>
<li><strong>缺点:</strong> 在样本数量较大的情况下,可能只用到了其中一部分数据就完成了训练,得到的只是局部最优解.另外,小批量样本的噪声较大,所以每次执行梯度下降,并不一定总是朝着最优的方向前进.<br><img src="./images/SGD%E4%B8%8EBGD.png" alt=""><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3>$$\left {<br>\begin{array}{l}{\boldsymbol{v}<em>{t}=-\eta \triangledown_\theta J\left(\boldsymbol{\theta}</em>{t}\right) } \ {\boldsymbol{\theta}<em>{t} =\boldsymbol{\theta}</em>{t-1}+\boldsymbol{v}_{t}}\end{array} \right.<br>$$<br>其中，$\eta$代表学习率，$\theta_t$表示$t$时刻的参数，$\triangledown _\theta J(\theta_t)$代表参数$\theta$在$t$时刻的导数，$v_t$代表参数的更新速度。<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2>在训练中，采取的策略与SGD一样，不同的是学习率的更新方式。动量的参数更新方式为：<br>$$\left {<br>\begin{array}{l}{\boldsymbol{v}<em>{t}=-\eta \triangledown_\theta J\left(\boldsymbol{\theta}</em>{t}\right)+\alpha \boldsymbol{v}<em>{t-1}} \ {\boldsymbol{\theta}</em>{t+1} =\boldsymbol{\theta}<em>{t}+\boldsymbol{v}</em>{t}}\end{array} \right.<br>$$<br>$\alpha$一般取值0.9.</li>
</ul>
<p>直观理解为：<br><img src="./images/momentum.jpg" alt=""><br>动量方法旨在加速学习（加快梯度的下降速度），特别是处理高曲率、小但一致的梯度，或是带噪声的梯度。动量算法累积了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。</p>
<table align=center>
    <tr>
        <td ><center><img height="200" width ="300" src="./images/momentum.gif" ></center></td>
    </tr>
</table>

<p>动量SGD算法引入$\alpha \boldsymbol{v}_{t-1}$使每一次的参数更新方向不仅仅取决于当前位置的梯度，还受到上一次参数更新方向的影响（如上图所示）。在某一维度上，当梯度方向不变时，更新速度变快，当梯度方向有所改变时，更新速度变慢，从而加快收敛速度，减少震荡。</p>
<p>带有动量的SGD的优点：</p>
<ul>
<li>加快收敛速度</li>
<li>抑制梯度下降时上下震荡的情况</li>
<li>通过局部极小点</li>
</ul>
<blockquote>
<p>分析：假设任意时刻参数的梯度均为$g_t=\triangledown_\theta J\left(\boldsymbol{\theta}_{t}\right)=g_0$，则使用SGD时,$t$时刻的梯度$g^{SGD}_t=g_0$,Momentum算法的梯度$g^{mom}_t=(\alpha^{t-1}+\alpha^{t-2}+…+\alpha+1)g_0=\frac{1-\alpha^{t}}{1-\alpha}g_0$.当$t\rightarrow +\infty$，因$\alpha&lt;1$,所以$\alpha^t\rightarrow 0$,所以$g_t^{mom}\rightarrow \frac{1}{1-\alpha}g_0$,当$\alpha=0.9$时，Momentum更新速度是SGD的10倍</p>
</blockquote>
<h2 id="Nesterov-NAG"><a href="#Nesterov-NAG" class="headerlink" title="Nesterov(NAG)"></a>Nesterov(NAG)</h2><p>Nesterov动量是Momentum的变种，即在计算参数梯度之前，前瞻一步，超前一个动量单位处：$\theta_t + \gamma v_{t-1}$,Nesterov动量可以理解为往Momentum动量中加入了一个校正因子。参数更新公式为：<br>$$\left {<br>\begin{array}{l}{\boldsymbol{v}<em>{t}=-\eta \triangledown_\theta J\left(\boldsymbol{\theta}</em>{t}+\gamma \boldsymbol{v}<em>{t-1}\right)+\alpha \boldsymbol{v}</em>{t-1}} \ {\boldsymbol{\theta}<em>{t+1} =\boldsymbol{\theta}</em>{t}+\boldsymbol{v}<em>{t}}\end{array} \right.<br>$$<br>Momentum动量与Nesterov动量的对比如下图所示，其中$\eta \triangledown_1$代表A节点 ($\theta_t$)的梯度，$\eta \triangledown_2$代表B节点($\theta</em>{t}+\gamma \boldsymbol{v}<em>{t-1}$的梯度)，灰色实线代表$t-1$时刻的速度$\alpha v</em>{t-1}$.</p>
<table align=center>
    <tr>
        <td ><center><img height="230" width ="250" src="./images/nesterov.png" ></center></td>
        <td ><center><img height="230" width ="300" src="./images/nesterov.bmp" ></center></td>
    </tr>
</table>

<blockquote>
<p>注意：图中的$\eta \triangledown_1以及\eta \triangledown_2$应该为$-\eta \triangledown_1$、$-\eta \triangledown_2$因为梯度方向是增长速度最快的方向，而图中所示为梯度的反方向，所以应该为$-\eta \triangledown_1$、$-\eta \triangledown_2$.</p>
</blockquote>
<p>Nesterov动量相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似，由于令了二阶导的信息，Nesterov动量算法才会比Momentum具有更快的收敛速度。</p>
<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>AdaGrad其实是对学习率进行了约束，AdaGrad独立地适应所有模型参数的学习率，缩放每个参数反比于其它所有梯度历史平方值总和的平方根。损失较大偏导的参数相应地拥有一个快速下降的学习率，而较小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。参数更新公式为：<br>$$\left {<br>\begin{array}{l}{<br>    g_t=\triangledown_\theta J(\theta_t)}\<br>    {n_t=n_{t-1}+g_t^2} \<br>    {\boldsymbol{v}<em>{t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}g_t } \ {\boldsymbol{\theta}</em>{t+1} =\boldsymbol{\theta}<em>{t}+\boldsymbol{v}</em>{t}}\end{array} \right.<br>$$<br>其中，$\epsilon$是个很小的数，用来保证分母非0。对$𝑔_𝑡$从1到t进行一个递推形成一个约束项regularizer——-$\frac{1}{\sqrt{n_t+\epsilon}}$。</p>
<p><strong>优点：</strong><br>前期$g_t$较小的时候， $\frac{1}{\sqrt{n_t+\epsilon}}$较大，梯度更新较大，可以解决SGD中学习率一直不变的问题;后期$g_t$较大的时候，$\frac{1}{\sqrt{n_t+\epsilon}}$较小，能够约束梯度.适合处理稀疏梯度.</p>
<p><strong>缺点：</strong><br>由公式可以看出，AdaGrad依赖于人工设置一个全局学习率𝜂，当$\eta$设置过大时，使regularizer过于敏感，对梯度的调节太大。在中后期，分母上梯度平方的累加将会越来越大，gradient→0，网络的更新能力会越来越弱，学习率会变的极小，使得训练提前结束。为了解决这样的问题，又提出了Adadelta算法。</p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adadelta是对AdaGrad的扩展，AgaGrad会累加所有历史梯度的平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。参数更新方式为：<br>$$\left {<br>\begin{array}{l}{<br>    g_t=\triangledown_\theta J(\theta_t)}\<br>    {n_t=\gamma n_{t-1}+(1-\gamma)g_t^2}\<br>    {\boldsymbol{v}<em>{t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}g_t } \ {\boldsymbol{\theta}</em>{t+1} =\boldsymbol{\theta}<em>{t}+\boldsymbol{v}</em>{t}}\end{array} \right.<br>$$<br>但是，此时Adadelta其实仍然依赖于全局学习率，因此，又做了一些处理，新的参数更新方式为：<br>$$\left {<br>\begin{array}{l}{<br>    g_t=\triangledown_\theta J(\theta_t)}\<br>    {E|g^2|<em>t=\rho \times E|g^2|</em>{t-1}+(1-\rho)\times g_t^2}\<br>    {v_t=-\frac{\sqrt{\sum_{r=1}^{t-1}}v_r}{\sqrt{E|t^2|<em>t+\epsilon}}g_t}\<br>    {\boldsymbol{\theta}</em>{t+1} =\boldsymbol{\theta}<em>{t}+\boldsymbol{v}</em>{t}}\end{array} \right.<br>$$<br>新的参数更新方式，不依赖于全局学习率，并且，训练初中期，加速效果不错，很快；训练后期，反复在局部最小值附近抖动。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop可以算作Adadelta的一个特例：当$\rho=0.5$时，$E|g^2|<em>t=\rho \times E|g^2|</em>{t-1}+(1-\rho)\times g_t^2$就变为了求梯度平方和的平均数。</p>
<p>如果再求根的话，就变成了RMS(均方根)：$RMS|g|<em>t=\sqrt{E|g^2|_t+\epsilon}$。RMSprop的参数更新方式为：<br>$$\left {<br>\begin{array}{l}{<br>    g_t=\triangledown_\theta J(\theta_t)}\<br>    {E|g^2|_t=\rho \times E|g^2|</em>{t-1}+(1-\rho)\times g_t^2} \<br>    {RMS|g|<em>t=\sqrt{E|g^2|_t+\epsilon}}\<br>    {\boldsymbol{v}</em>{t}=-\frac{\eta}{RMS|g|<em>t }g_t} \ {\boldsymbol{\theta}</em>{t+1} =\boldsymbol{\theta}<em>{t}+\boldsymbol{v}</em>{t}}\end{array} \right.<br>$$<br><strong>特点：</strong> (1)RMSprop依然依赖于全局学习率;(2)RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间;(3)适合处理非平稳目标 - 对于RNN效果很好</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam优化器结合了AdaGrad与RMSProp两种算法的优点。对梯度的一阶距估计$m_t$（即梯度的均值）和二阶距估计$n_t$（即梯度的未中心化的方差）进行综合考虑，计算出更新步长。更新方式为：<br>$$\left {<br>\begin{array}{l}{<br>    g_t=\triangledown_\theta J(\theta_t)}\<br>    {m_t=\beta_1m_{t-1}+(1-\beta_1)g_t}\<br>    {v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2}\<br>    {\hat{m_t}=\frac{m_t}{1-\beta_1^t}}\<br>    {\hat{v}<em>t=\frac{v_t}{1-\beta_2^t}}\<br>    {\boldsymbol{\theta}</em>{t+1}=\boldsymbol{\theta}_{t}-\frac{\eta}{\sqrt{\hat{v}_t+\epsilon}}\hat{m}_t}\<br>\end{array} \right.<br>$$</p>
<blockquote>
<p>注：所有的$t$均表示$t$时刻。$m_t$、$n_t$分别是梯度的一阶距估计和二阶距估计，可以看做是对期望$E|g_t|、E|g_t^2|$的估计；$\hat{m}_t、\hat{n}_t$是对$m_t$、$n_t$的校正，这样可以近似为对期望的无偏估计。</p>
</blockquote>
<p><strong>优点：</strong></p>
<ol>
<li>实现简单，计算高效，对内存需求少</li>
<li>参数的更新不受梯度的伸缩变换影响</li>
<li>超参数具有很好的解释性，且通常无需调整或仅需很少的微调</li>
<li>更新的步长能够被限制在大致的范围内（初始学习率）</li>
<li>能自然地实现步长退火过程（自动调整学习率）</li>
<li>很适合应用于大规模的数据及参数的场景</li>
<li>适用于不稳定目标函数</li>
<li>适用于梯度稀疏或梯度存在很大噪声的问题</li>
</ol>
<blockquote>
<p>参考资料：</p>
<p><a href="https://zhuanlan.zhihu.com/p/73264637" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73264637</a> </p>
<p><a href="https://zhuanlan.zhihu.com/p/60088231" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60088231</a></p>
<p><a href="https://blog.csdn.net/u012759136/article/details/52302426" target="_blank" rel="noopener">https://blog.csdn.net/u012759136/article/details/52302426</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://chenzhongyao.github.io/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" data-id="ckbku3uo4000absva11auazun" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          深度学习中的正则化
        
      </div>
    </a>
  
  
    <a href="/2020/06/18/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E5%88%B7%E8%BF%87%E7%9A%84%E5%89%91%E6%8C%87offer/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">那些年，刷过的剑指offer</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/18/CNN%E7%9A%84%E5%89%8D%E5%90%91%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">CNN的前向与反向传播</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/">深度学习中的正则化</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">深度学习中的模型优化</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E5%88%B7%E8%BF%87%E7%9A%84%E5%89%91%E6%8C%87offer/">那些年，刷过的剑指offer</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/">拉格朗日对偶性</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>