<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>svd分解推导过程 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="SVD奇异值分解逐步推导1. 回顾特征值和特征向量首先回顾下特征值和特征向量的定义：$$Ax&#x3D;\lambda x$$其中，A是一个$n\times n$的矩阵，$x$是一个$n$维向量，则$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$对应的特征向量。 求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$\lambda">
<meta property="og:type" content="article">
<meta property="og:title" content="svd分解推导过程">
<meta property="og:url" content="https://chenzhongyao.github.io/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="SVD奇异值分解逐步推导1. 回顾特征值和特征向量首先回顾下特征值和特征向量的定义：$$Ax&#x3D;\lambda x$$其中，A是一个$n\times n$的矩阵，$x$是一个$n$维向量，则$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$对应的特征向量。 求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$\lambda">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-06-18T13:41:36.870Z">
<meta property="article:modified_time" content="2019-08-04T12:49:32.113Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://chenzhongyao.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-svd分解推导过程" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/" class="article-date">
  <time datetime="2020-06-18T13:41:36.870Z" itemprop="datePublished">2020-06-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      svd分解推导过程
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="SVD奇异值分解逐步推导"><a href="#SVD奇异值分解逐步推导" class="headerlink" title="SVD奇异值分解逐步推导"></a>SVD奇异值分解逐步推导</h2><h3 id="1-回顾特征值和特征向量"><a href="#1-回顾特征值和特征向量" class="headerlink" title="1. 回顾特征值和特征向量"></a>1. 回顾特征值和特征向量</h3><p>首先回顾下特征值和特征向量的定义：<br>$$<br>Ax=\lambda x<br>$$<br>其中，A是一个$n\times n$的矩阵，$x$是一个$n$维向量，则$\lambda$是矩阵A的一个特征值，而$x$是矩阵A的特征值$\lambda$对应的特征向量。</p>
<p>求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$\lambda _1\leq \lambda_2\leq … \leq\lambda_n$，以及这$n$个特征值所对应的特征向量$w_1, w_2, …, w_n$,那么矩阵A就可以用以下的特征分解表示：<br>$$<br>W^{-1}AW=\Lambda \Leftrightarrow A=W\Lambda W^{-1}<br>$$<br>其中，$W$是这$n$个特征向量所组成的$n\times n$维矩阵，而$\Lambda$是将这$n$个特征值作为主对角线的$n\times n$维矩阵。一般情况下，我们会把$W$的这$n$个特征向量标准化，即满足$||w_i||_2=1$，或者$w_i^Tw_i=1$,此时$W$的$n$个特征向量为标准正交基，满足$W^TW=I$，即$W^T=W^{-1}$,也就是说$W$为酉矩阵。这样我们的特征分解表达式可以写成：<br>$$<br>A=W\Lambda W^T<br>$$</p>
<blockquote>
<p>题外延伸——矩阵压缩：</p>
<p>设$W=(w_1, w_2, w_3,…,w_n)$，则:<br>$$W^T=\begin{pmatrix}<br> w_1^T\<br> w_2^T\<br> w_3^T\<br>… \<br>w_n^T<br>\end{pmatrix}<br>$$<br>那么:<br>$$<br>A=(w_1, w_2, w_3,…,w_n)\begin{pmatrix}<br>    \lambda_1&amp;&amp;&amp;\<br>    &amp;\lambda_2&amp;&amp;\<br>    &amp;&amp;\lambda_3&amp;\<br>    &amp;&amp;&amp;…\<br>    &amp;&amp;&amp;&amp;\lambda_n<br>\end{pmatrix}\begin{pmatrix}<br> w_1^T\<br> w_2^T\<br> w_3^T\<br>… \<br>w_n^T<br>\end{pmatrix}<br>\<br>=\lambda_1w_1w_1^T+\lambda_2w_2w_2^T+\lambda_3w_3w_3^T+…+\lambda_nw_nw_n^T<br>$$</p>
<p>假设A为$n\times n$维矩阵，如果正常表示矩阵A共需使用$n^2$个元素，如果将取得的特征值$\lambda_1,\lambda_2,\lambda_3,…,\lambda_n$按从大到小排序，即$\lambda_1\geq\lambda_2\geq\lambda_3\geq…\geq\lambda_n$，则将A的压缩表示为$\lambda_1w_1w_1^T$，即只需要$n+1$个元素。</p>
</blockquote>
<p>注意到要进行特征分解，矩阵A必须为方阵。</p>
<p>那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。</p>
<h3 id="2-SVD推导"><a href="#2-SVD推导" class="headerlink" title="2. SVD推导"></a>2. SVD推导</h3><h4 id="Step1：矩阵分解"><a href="#Step1：矩阵分解" class="headerlink" title="Step1：矩阵分解"></a>Step1：矩阵分解</h4><p>假如$A$为$m\times n$维矩阵，则$A^TA$为对称正定矩阵。</p>
<blockquote>
<p>证明：1)对称性：$(A^TA)^T=A^TA\Rightarrow 对称性$<br>2)正定性： $x^TA^TAx=(Ax)^T(Ax)\geq 0\Rightarrow正定性$</p>
</blockquote>
<p>对于矩阵A，有$(A^TA)v_i=\lambda _iv_i$，其中$\lambda_i$为特征值，$v_i$为特征向量。假定$(v_i, v_j)$是一组正交基，那么有$v_i^T\cdot v_j=0$，那么：<br>$$<br>(Av_i, Av_j)=(Av_i)^T\cdot Av_j=v_i^TA^TAv_j=v_i^T\lambda_jv_j=\lambda_jv_i^Tv_j=0<br>$$<br>因此，$Av_i,Av_j$也是一组正交基，根据上述公式可以推导出$(Av_i, Av_i)=\lambda_iv_i^Tv_i=\lambda_i$,从而可以得到：<br>$$<br>|Av_i|^2=\lambda_i<br>$$<br>$$<br>|Av_i|=\sqrt{\lambda_i}<br>$$<br>根据上述公式，有$\frac{Av_i}{|Av_i|}=\frac{1}{\sqrt{\lambda_i}}Av_i$，令$\frac{1}{\sqrt{\lambda_i}}Av_i=u_i$，可得：<br>$$<br>Av_i=\sqrt{\lambda_i}u_i=\delta_i u_i<br>$$<br>其中，$\delta_i=\sqrt{\lambda_i}$，进一步推导：<br>$$<br>AV=A(v_1,v_2,…,v_n)=(Av_1,Av_2,…,Av_n)=(\delta_1u_1,\delta_2u_2,…,\delta_nu_n)=U\Sigma<br>$$<br>从而得出：<br>$$A=U\Sigma V^T$$</p>
<h4 id="Step2-矩阵计算"><a href="#Step2-矩阵计算" class="headerlink" title="Step2:矩阵计算"></a>Step2:矩阵计算</h4><p>得到矩阵A的表示后，我们应该如何计算向量$U$和$V$呢？继续往下面分析：</p>
<p>首先计算出A的转置$A^T$：$A^T=V\Sigma ^TU^T$<br>$$<br>A^TA=V\Sigma^TU^TU\Sigma V^T=V\Sigma^2V^T<br>$$<br>利用上式可以得到，$A^TAv_i=\lambda_iv_i$，只需要求出$A^TA$的特征向量即可得到$V$.</p>
<p>同理可得$AA^T$的值：<br>$$<br>AA^T=U\Sigma V^TV\Sigma^TU^T=U\Sigma^2U^T<br>$$<br>可以得到$AA^Tu_i=\lambda_iu_i$，只需要求出$AA^T$的特征向量即可得到$U$.</p>
<blockquote>
<p>题外延伸—–矩阵(图像)压缩：</p>
<p>一个$m\times n$的矩阵A经SVD分解后，可以写成如下形式：<br>$$<br>A_{m\times n}=U_{m\times m}\Sigma V^T_{n\times n}=(u_1,u_2,…,u_m)\begin{pmatrix}<br>    \lambda_1^{\frac{1}{2}}&amp;&amp;\<br>    &amp;\lambda_2^{\frac{1}{2}}&amp;\<br>    &amp;&amp;…<br>\end{pmatrix}\begin{pmatrix}<br>    v_1^T\<br>    v_2^T\<br>    …\<br>    v_n^T<br>\end{pmatrix}\<br>=\lambda_1^{\frac{1}{2}}u_1v_1^T+\lambda_2^{\frac{1}{2}}u_2v_2^T+…<br>$$<br>假设A为$m\times n$维矩阵，在没有压缩时表示矩阵A共需要$m\times n$个元素。如果将取得的特征值按从大到小排序，即$\lambda_1\geq\lambda_2\geq\lambda_3\geq…\geq\lambda_{min{m,n}}$，则A的压缩最小压缩表示为$\lambda_1^{\frac{1}{2}}u_1v_1^T$，即需要$m+n+1$个元素。<br>当压缩储存量为$(m+n+1)\times k$时，误差为<br>$$<br>error=\frac{(m+n)\times\sum_{i=1}^{k}\lambda_i}{(m+n)\times\sum_{i=1}^{min(m,n)}\lambda_i}=\frac{\sum_{i=1}^{k}\lambda_i}{\sum_{i=1}^{min(m,n)}\lambda_i}<br>$$</p>
</blockquote>
<h3 id="例题讲解"><a href="#例题讲解" class="headerlink" title="例题讲解"></a>例题讲解</h3><p>我们举一个简单的例子讲解矩阵时如何进行奇异值分解的。定义矩阵A为：<br>$$<br>A=\begin{pmatrix}<br>0 &amp;1 \<br>1&amp;1\<br>1&amp;0<br>\end{pmatrix}<br>$$<br>首先求出$A^TA、AA^T$：<br>$$<br>A^TA=\begin{pmatrix}<br>    0&amp;1&amp;1\<br>    1&amp;1&amp;0<br>\end{pmatrix}\begin{pmatrix}<br>    0&amp;1\<br>    1&amp;1\<br>    1&amp;0<br>\end{pmatrix}=\begin{pmatrix}<br>    2&amp;1\<br>    1&amp;2<br>\end{pmatrix}<br>$$<br>$$<br>AA^T=\begin{pmatrix}<br>    0&amp;1\<br>    1&amp;1\<br>    1&amp;0<br>\end{pmatrix}\begin{pmatrix}<br>    0&amp;1&amp;1\<br>    1&amp;1&amp;0<br>\end{pmatrix}=\begin{pmatrix}<br>    1&amp;1&amp;0\<br>    1&amp;2&amp;1\<br>    0&amp;1&amp;1<br>\end{pmatrix}<br>$$<br>进而求出$A^TA$的特征值和特征向量：<br>$$<br>\lambda_1=3;v_1=\begin{pmatrix}<br>    \frac{1}{\sqrt{2}}\<br>    \frac{1}{\sqrt{2}}<br>\end{pmatrix};<br>\lambda_2=1;v_2=\begin{pmatrix}<br>    -\frac{1}{\sqrt{2}}\<br>    \frac{1}{\sqrt{2}}<br>\end{pmatrix}<br>$$<br>接着求出$AA^T$的特征值和特征向量：<br>$$<br>\lambda_1=3;u_1=\begin{pmatrix}<br>    \frac{1}{\sqrt{6}}\<br>    \frac{2}{\sqrt{6}}\<br>    \frac{1}{\sqrt{6}}<br>\end{pmatrix};<br>\lambda_2=1;u_2=\begin{pmatrix}<br>    \frac{1}{\sqrt{2}}\<br>    0\<br>    -\frac{1}{\sqrt{2}}<br>\end{pmatrix};\lambda_3=0;u_3=\begin{pmatrix}<br>    \frac{1}{\sqrt{3}}\<br>    -\frac{1}{\sqrt{3}}\<br>    \frac{1}{\sqrt{3}}<br>\end{pmatrix}<br>$$<br>利用$Av_i=\delta_iu_i,i=1,2$求奇异值：<br>$$<br>\begin{pmatrix}<br>    0&amp;1\<br>    1&amp;1\<br>    1&amp;0<br>\end{pmatrix}\begin{pmatrix}<br>    \frac{1}{\sqrt{2}}\<br>    \frac{1}{\sqrt{2}}<br>\end{pmatrix}=\delta_1\begin{pmatrix}<br>    \frac{1}{\sqrt{6}}\<br>    \frac{2}{\sqrt{6}}\<br>    \frac{1}{\sqrt{6}}<br>\end{pmatrix} \Rightarrow\delta_1=\sqrt{3}<br>$$<br>$$<br>\begin{pmatrix}<br>    0&amp;1\<br>    1&amp;1\<br>    1&amp;0<br>\end{pmatrix}\begin{pmatrix}<br>    -\frac{1}{\sqrt{2}}\<br>    \frac{1}{\sqrt{2}}<br>\end{pmatrix}=\delta_2\begin{pmatrix}<br>    \frac{1}{\sqrt{2}}\<br>    0\<br>    -\frac{1}{\sqrt{2}}<br>\end{pmatrix} \Rightarrow\delta_2=1<br>$$<br>也可以用$\delta_i=\sqrt{\lambda_i}$直接求出奇异值为$\sqrt{3}$和$1$.</p>
<p>最终得到矩阵A的奇异值分解为：<br>$$<br>A=U\Sigma V^T=\begin{pmatrix}<br>    \frac{1}{\sqrt{6}}  &amp;\frac{1}{\sqrt{2}}  &amp; \frac{1}{\sqrt{3}} \<br>    \frac{2}{\sqrt{6}}  &amp;0  &amp; -\frac{1}{\sqrt{3}} \<br>    \frac{1}{\sqrt{6}}  &amp;-\frac{1}{\sqrt{2}}  &amp; \frac{1}{\sqrt{3}}<br>\end{pmatrix}\begin{pmatrix}<br>    \sqrt{3}  &amp; 0\<br>    0  &amp; 1\<br>    0  &amp; 0<br>\end{pmatrix}\begin{pmatrix}<br>    \frac{1}{\sqrt{2}}  &amp; \frac{1}{\sqrt{2}} \<br>    -\frac{1}{\sqrt{2}}  &amp; \frac{1}{\sqrt{2}}<br>\end{pmatrix}<br>$$</p>
<h3 id="SVD的一些性质"><a href="#SVD的一些性质" class="headerlink" title="SVD的一些性质"></a>SVD的一些性质</h3><p>对于奇异值，他跟我们特征分解中的特征值类似，在奇艺置矩阵中也是按照从大到小排列，而且奇异值的减少特别快，在很多情况下，前10%甚至1%的奇异值就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个奇异值和对应的左右奇异向量来近似描述矩阵(与前面描述的题外延伸之矩阵压缩类似)，由于这个重要的性质，SVD也可以用于PCA降维，来做数据压缩和去噪，也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需要来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
<blockquote>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/29846048" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29846048</a> </p>
<p>参考：<a href="https://www.csuldw.com/2017/03/09/2017-03-09-svd/" target="_blank" rel="noopener">https://www.csuldw.com/2017/03/09/2017-03-09-svd/</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://chenzhongyao.github.io/2020/06/18/svd%E5%88%86%E8%A7%A3%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B/" data-id="ckbku3uny0004bsvaez673vu1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/06/18/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          反向传播算法推导
        
      </div>
    </a>
  
  
    <a href="/2020/06/18/RNN%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">RNN前向传播、反向传播与并行计算</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/18/CNN%E7%9A%84%E5%89%8D%E5%90%91%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">CNN的前向与反向传播</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/">深度学习中的正则化</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">深度学习中的模型优化</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E9%82%A3%E4%BA%9B%E5%B9%B4%EF%BC%8C%E5%88%B7%E8%BF%87%E7%9A%84%E5%89%91%E6%8C%87offer/">那些年，刷过的剑指offer</a>
          </li>
        
          <li>
            <a href="/2020/06/18/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/">拉格朗日对偶性</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>